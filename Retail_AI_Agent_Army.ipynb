{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "# Note: In a local environment, you would run these commands in your terminal.\n",
        "# In a Jupyter/Colab notebook, the '!' prefix executes them as shell commands.\n",
        "try:\n",
        "    import langchain, langgraph, chromadb, sentence_transformers, lightgbm, pandas, numpy, matplotlib, sklearn, asyncio, aiohttp, langsmith, mcp, guardrails\n",
        "    print(\"‚úÖ All packages already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required packages...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain\", \"langchain-openai\", \"langgraph\", \"langchain-community\", \"chromadb\", \"sentence-transformers\", \"lightgbm\", \"pandas\", \"numpy\", \"matplotlib\", \"scikit-learn\", \"asyncio\", \"aiohttp\", \"langsmith\", \"mcp-client\", \"guardrails-ai\"])\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# LangSmith for AI Observability\n",
        "import langsmith\n",
        "from langsmith import Client, traceable, run_trees\n",
        "\n",
        "# MCP (Model Context Protocol) imports\n",
        "# This is kept for conceptual context, though no specific client functions are called.\n",
        "import mcp\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 3: Configure API Keys and Observability\n",
        "print(\"üîë Setting up API Keys and Observability...\")\n",
        "\n",
        "# It's highly recommended to use environment variables or a secrets management tool.\n",
        "# For this example, we'll use placeholders.\n",
        "# IMPORTANT: Replace these with your actual keys.\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', \"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\")\n",
        "LANGSMITH_API_KEY = os.environ.get('LANGSMITH_API_KEY', \"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\")\n",
        "LANGSMITH_PROJECT = os.environ.get('LANGSMITH_PROJECT', \"retail-agent-pool\")\n",
        "\n",
        "# Set environment variables for LangChain and LangSmith\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # Enables LangSmith tracing\n",
        "\n",
        "# Initialize LangSmith client\n",
        "try:\n",
        "    if LANGSMITH_API_KEY != \"YOUR_LANGSMITH_API_KEY\":\n",
        "        langsmith_client = Client()\n",
        "        print(\"‚úÖ LangSmith client initialized successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è LangSmith API key not set. Skipping client initialization.\")\n",
        "        langsmith_client = None\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è LangSmith initialization warning: {e}\")\n",
        "    langsmith_client = None\n",
        "\n",
        "# Test the OpenAI API key\n",
        "try:\n",
        "    if OPENAI_API_KEY != \"YOUR_OPENAI_API_KEY\":\n",
        "        test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        test_response = test_llm.invoke(\"Test\")\n",
        "        print(\"‚úÖ OpenAI API key verified successfully!\")\n",
        "    else:\n",
        "         print(\"‚ùå OpenAI API key is not set. Please replace 'YOUR_OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API key error: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: MCP Server Implementation for Retail\n",
        "class RetailMCPServer:\n",
        "    \"\"\"\n",
        "    A simulated Model Context Protocol (MCP) server that provides specialized tools\n",
        "    for retail analysis. Each tool is traceable for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"price_optimizer\": self.price_optimization_tool,\n",
        "            \"demand_forecaster\": self.demand_forecasting_tool,\n",
        "            \"inventory_optimizer\": self.inventory_optimization_tool,\n",
        "            \"customer_analyzer\": self.customer_analytics_tool,\n",
        "            \"competitor_monitor\": self.competitive_intelligence_tool,\n",
        "            \"market_analyzer\": self.market_analysis_tool\n",
        "        }\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "\n",
        "    @traceable\n",
        "    async def price_optimization_tool(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced price optimization using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Perform advanced price optimization analysis:\n",
        "\n",
        "            PRODUCT: {product_data}\n",
        "            MARKET: {market_conditions}\n",
        "\n",
        "            Calculate:\n",
        "            1. Price elasticity estimation\n",
        "            2. Optimal price point with confidence intervals\n",
        "            3. Competitive positioning strategy\n",
        "            4. Revenue maximization approach\n",
        "            5. Risk assessment\n",
        "\n",
        "            Return structured JSON analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"price_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": response.content,\n",
        "                \"recommendations\": {\n",
        "                    \"optimal_price_range\": {\"min\": 45.0, \"max\": 125.0, \"recommended\": 89.99},\n",
        "                    \"confidence_score\": 0.87,\n",
        "                    \"expected_revenue_impact\": \"+15-25%\",\n",
        "                    \"risk_level\": \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"price_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def demand_forecasting_tool(self, historical_data: List, market_signals: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Demand forecasting with market signals.\"\"\"\n",
        "        try:\n",
        "            forecast_data = {\n",
        "                \"next_30_days\": np.random.normal(100, 20, 30).tolist(),\n",
        "                \"confidence_interval\": {\"lower\": 80, \"upper\": 120},\n",
        "                \"seasonality_factor\": 1.15,\n",
        "                \"trend_direction\": \"upward\"\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"demand_forecaster\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"forecast\": forecast_data,\n",
        "                \"market_signals_used\": list(market_signals.keys()),\n",
        "                \"data_points_analyzed\": len(historical_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"demand_forecaster\"}\n",
        "\n",
        "    @traceable\n",
        "    async def inventory_optimization_tool(self, current_stock: Dict, demand_forecast: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Inventory optimization based on demand forecasting.\"\"\"\n",
        "        try:\n",
        "            optimization_recommendations = {\n",
        "                \"safety_stock_levels\": {\n",
        "                    \"high_demand\": current_stock.get('current_stock', 0) * 0.3,\n",
        "                    \"medium_demand\": current_stock.get('current_stock', 0) * 0.2,\n",
        "                    \"low_demand\": current_stock.get('current_stock', 0) * 0.1\n",
        "                },\n",
        "                \"reorder_points\": {\n",
        "                    \"urgent\": current_stock.get('reorder_point', 0) * 0.8,\n",
        "                    \"standard\": current_stock.get('reorder_point', 0),\n",
        "                    \"relaxed\": current_stock.get('reorder_point', 0) * 1.2\n",
        "                },\n",
        "                \"cost_optimization\": {\n",
        "                    \"expected_savings\": \"15-20%\",\n",
        "                    \"stockout_risk_reduction\": \"40-60%\",\n",
        "                    \"carrying_cost_optimization\": \"10-15%\"\n",
        "                }\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"inventory_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"optimization\": optimization_recommendations,\n",
        "                \"demand_forecast_incorporated\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"inventory_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def customer_analytics_tool(self, customer_data: Dict, purchase_history: List) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced customer analytics using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze customer behavior and generate insights:\n",
        "\n",
        "            CUSTOMER: {customer_data}\n",
        "            PURCHASE HISTORY: {purchase_history[:5]}\n",
        "\n",
        "            Provide:\n",
        "            1. Customer lifetime value prediction\n",
        "            2. Churn risk assessment\n",
        "            3. Personalization opportunities\n",
        "            4. Retention strategy\n",
        "            5. Cross-selling recommendations\n",
        "\n",
        "            Return structured analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"customer_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"customer_insights\": response.content,\n",
        "                \"metrics\": {\n",
        "                    \"lifetime_value_score\": self._calculate_ltv(purchase_history),\n",
        "                    \"churn_risk\": \"low\" if len(purchase_history) > 3 else \"medium\",\n",
        "                    \"engagement_level\": \"high\" if len(purchase_history) > 5 else \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"customer_analyzer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def competitive_intelligence_tool(self, product_data: Dict, market_scope: str = \"local\") -> Dict:\n",
        "        \"\"\"MCP Tool: Competitive market intelligence.\"\"\"\n",
        "        try:\n",
        "            competitor_analysis = {\n",
        "                \"main_competitors\": [\"Competitor A\", \"Competitor B\", \"Competitor C\"],\n",
        "                \"price_comparison\": {\n",
        "                    \"our_price\": product_data.get('current_price', 0),\n",
        "                    \"competitor_avg\": product_data.get('current_price', 0) * 0.9,\n",
        "                    \"market_range\": {\n",
        "                        \"min\": product_data.get('current_price', 0) * 0.7,\n",
        "                        \"max\": product_data.get('current_price', 0) * 1.3\n",
        "                    }\n",
        "                },\n",
        "                \"competitive_positioning\": \"market_leader\" if product_data.get('current_price', 0) > 100 else \"value_player\",\n",
        "                \"recommendations\": [\n",
        "                    \"Monitor competitor pricing weekly\",\n",
        "                    \"Differentiate through value-added services\",\n",
        "                    \"Consider bundle pricing strategies\"\n",
        "                ]\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"competitor_monitor\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": competitor_analysis,\n",
        "                \"market_scope\": market_scope\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"competitor_monitor\"}\n",
        "\n",
        "    @traceable\n",
        "    async def market_analysis_tool(self, product_category: str, timeframe: str = \"30d\") -> Dict:\n",
        "        \"\"\"MCP Tool: Comprehensive market analysis using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze market conditions for {product_category} over {timeframe}:\n",
        "\n",
        "            Provide:\n",
        "            1. Market trends and growth projections\n",
        "            2. Consumer behavior insights\n",
        "            3. Regulatory considerations\n",
        "            4. Technology impacts\n",
        "            5. Strategic recommendations\n",
        "\n",
        "            Focus on actionable insights for retail pricing and inventory.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"market_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"market_analysis\": response.content,\n",
        "                \"timeframe\": timeframe,\n",
        "                \"category\": product_category\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"market_analyzer\"}\n",
        "\n",
        "    def _calculate_ltv(self, purchase_history: List) -> float:\n",
        "        \"\"\"Helper function to calculate customer lifetime value.\"\"\"\n",
        "        if not purchase_history:\n",
        "            return 0.0\n",
        "        total_revenue = sum(p.get('revenue', 0) for p in purchase_history)\n",
        "        purchase_count = len(purchase_history)\n",
        "        avg_order_value = total_revenue / purchase_count if purchase_count > 0 else 0\n",
        "        return min(round(avg_order_value * purchase_count * 0.1, 2), 1000.0)\n",
        "\n",
        "print(\"üõ†Ô∏è Initializing MCP Server...\")\n",
        "mcp_server = RetailMCPServer()\n",
        "\n",
        "\n",
        "# Step 5: A2A (Agent-to-Agent) Communication Layer\n",
        "class A2ACommunicationLayer:\n",
        "    \"\"\"\n",
        "    Manages communication between agents, including direct messages, broadcasts,\n",
        "    and consensus requests, with full tracing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agent_registries = {}\n",
        "        self.conversation_history = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    @traceable\n",
        "    async def register_agent(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        \"\"\"Register an agent in the A2A network.\"\"\"\n",
        "        self.agent_registries[agent_id] = {\n",
        "            \"agent_type\": agent_type,\n",
        "            \"capabilities\": capabilities,\n",
        "            \"status\": \"active\",\n",
        "            \"last_heartbeat\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.performance_metrics[agent_id] = {\n",
        "            \"messages_sent\": 0, \"messages_received\": 0,\n",
        "            \"response_time_avg\": 0.0, \"error_rate\": 0.0\n",
        "        }\n",
        "        print(f\"‚úÖ Agent registered: {agent_id} ({agent_type})\")\n",
        "\n",
        "    @traceable\n",
        "    async def send_message(self, from_agent: str, to_agent: str, message: Dict, message_type: str = \"direct\"):\n",
        "        \"\"\"Send a message between agents with tracing.\"\"\"\n",
        "        try:\n",
        "            message_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.now().isoformat()\n",
        "            run_tree = langsmith.get_run_tree()\n",
        "            trace_id = run_tree.id if run_tree else 'unknown'\n",
        "\n",
        "            message_package = {\n",
        "                \"message_id\": message_id, \"from_agent\": from_agent,\n",
        "                \"to_agent\": to_agent, \"message_type\": message_type,\n",
        "                \"content\": message, \"timestamp\": timestamp, \"trace_id\": trace_id\n",
        "            }\n",
        "\n",
        "            if from_agent not in self.conversation_history:\n",
        "                self.conversation_history[from_agent] = []\n",
        "            self.conversation_history[from_agent].append(message_package)\n",
        "\n",
        "            self.performance_metrics[from_agent][\"messages_sent\"] += 1\n",
        "            if to_agent in self.performance_metrics:\n",
        "                self.performance_metrics[to_agent][\"messages_received\"] += 1\n",
        "\n",
        "            print(f\"üì® A2A Message: {from_agent} ‚Üí {to_agent} ({message_type})\")\n",
        "            return message_id\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Message Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @traceable\n",
        "    async def broadcast_message(self, from_agent: str, message: Dict, agent_filter: List[str] = None):\n",
        "        \"\"\"Broadcast a message to multiple agents.\"\"\"\n",
        "        try:\n",
        "            target_agents = agent_filter if agent_filter else list(self.agent_registries.keys())\n",
        "            target_agents = [agent for agent in target_agents if agent != from_agent]\n",
        "\n",
        "            tasks = [self.send_message(from_agent, agent_id, message, \"broadcast\") for agent_id in target_agents]\n",
        "            message_ids = await asyncio.gather(*tasks)\n",
        "            return message_ids\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Broadcast Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_communication_metrics(self) -> Dict:\n",
        "        \"\"\"Get A2A communication performance metrics.\"\"\"\n",
        "        total_messages = sum(m[\"messages_sent\"] for m in self.performance_metrics.values())\n",
        "        active_agents = len([a for a, i in self.agent_registries.items() if i[\"status\"] == \"active\"])\n",
        "        return {\n",
        "            \"total_agents_registered\": len(self.agent_registries),\n",
        "            \"active_agents\": active_agents,\n",
        "            \"total_messages_exchanged\": total_messages,\n",
        "            \"agent_performance\": self.performance_metrics,\n",
        "            \"system_health\": \"optimal\" if active_agents > 0 else \"degraded\"\n",
        "        }\n",
        "\n",
        "print(\"üîÑ Initializing A2A Communication Layer...\")\n",
        "a2a_communication = A2ACommunicationLayer()\n",
        "\n",
        "\n",
        "# Step 6: Enhanced Vector Database with Observability\n",
        "class ObservableVectorDB:\n",
        "    \"\"\"\n",
        "    A wrapper for a Chroma vector database that adds observability and performance tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_store = None\n",
        "        self.query_metrics = {\"total_queries\": 0, \"successful_queries\": 0, \"average_response_time\": 0.0}\n",
        "        self.setup_sample_data()\n",
        "\n",
        "    @traceable\n",
        "    def setup_sample_data(self):\n",
        "        \"\"\"Initialize with comprehensive retail sample data.\"\"\"\n",
        "        sample_docs = [\n",
        "            Document(page_content=\"Premium pricing strategy works best for electronics with 40-60% markup\", metadata={\"category\": \"pricing\", \"product_type\": \"electronics\"}),\n",
        "            Document(page_content=\"Competitive pricing essential for smartphone accessories market\", metadata={\"category\": \"pricing\", \"product_type\": \"accessories\"}),\n",
        "            Document(page_content=\"Safety stock levels should cover 2-week demand for fast-moving goods\", metadata={\"category\": \"inventory\", \"metric\": \"safety_stock\"}),\n",
        "            Document(page_content=\"Holiday season demand increases 60-80% for electronics\", metadata={\"category\": \"trends\", \"season\": \"Q4\"}),\n",
        "            Document(page_content=\"Price sensitivity is highest in the budget segment\", metadata={\"category\": \"customer\", \"insight\": \"pricing\"})\n",
        "        ]\n",
        "        self.vector_store = Chroma.from_documents(documents=sample_docs, embedding=self.embeddings, persist_directory=\"./chroma_retail_db\")\n",
        "        print(\"‚úÖ Observable Vector database initialized\")\n",
        "\n",
        "    @traceable\n",
        "    async def enhanced_search(self, query: str, category: str = None, k: int = 3, agent_id: str = \"unknown\") -> List[Dict]:\n",
        "        \"\"\"Enhanced semantic search with observability.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.query_metrics[\"total_queries\"] += 1\n",
        "        try:\n",
        "            filter_dict = {\"category\": category} if category else {}\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.query_metrics[\"successful_queries\"] += 1\n",
        "            total_queries = self.query_metrics[\"successful_queries\"]\n",
        "            self.query_metrics[\"average_response_time\"] = ((self.query_metrics[\"average_response_time\"] * (total_queries - 1)) + response_time) / total_queries\n",
        "\n",
        "            formatted_results = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"similarity_score\": float(score)} for doc, score in results]\n",
        "            print(f\"üîç VectorDB Search: '{query}' ‚Üí {len(formatted_results)} results\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå VectorDB Search Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Get vector database performance metrics.\"\"\"\n",
        "        success_rate = self.query_metrics[\"successful_queries\"] / max(1, self.query_metrics[\"total_queries\"])\n",
        "        return {**self.query_metrics, \"success_rate\": success_rate, \"health_status\": \"optimal\"}\n",
        "\n",
        "print(\"üõ¢Ô∏è Initializing Observable Vector Database...\")\n",
        "observable_vector_db = ObservableVectorDB()\n",
        "\n",
        "\n",
        "# Step 7: Enhanced Base Agent with MCP and A2A Integration\n",
        "class EnhancedRetailAgent:\n",
        "    \"\"\"\n",
        "    A base class for retail AI agents, integrating MCP tools, A2A communication,\n",
        "    and an observable vector database.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str, agent_type: str, model: str = \"gpt-4\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = ChatOpenAI(model=model, temperature=0.1)\n",
        "        self.mcp_server = mcp_server\n",
        "        self.a2a = a2a_communication\n",
        "        self.vector_db = observable_vector_db\n",
        "        self.history = []\n",
        "        self.performance_metrics = {\n",
        "            \"tasks_completed\": 0, \"mcp_tools_used\": 0,\n",
        "            \"a2a_messages_sent\": 0, \"vector_db_queries\": 0,\n",
        "            \"avg_response_time\": 0.0, \"success_rate\": 1.0\n",
        "        }\n",
        "        asyncio.create_task(self._register_with_a2a())\n",
        "\n",
        "    async def _register_with_a2a(self):\n",
        "        \"\"\"Register agent with the A2A communication layer.\"\"\"\n",
        "        await self.a2a.register_agent(self.agent_id, self.agent_type, self._get_agent_capabilities())\n",
        "\n",
        "    def _get_agent_capabilities(self) -> List[str]:\n",
        "        \"\"\"Define agent capabilities for A2A registration.\"\"\"\n",
        "        caps = [\"analysis\", \"decision_support\", \"collaboration\"]\n",
        "        if \"pricing\" in self.agent_type: caps.extend([\"price_optimization\", \"competitive_analysis\"])\n",
        "        elif \"inventory\" in self.agent_type: caps.extend([\"demand_forecasting\", \"stock_optimization\"])\n",
        "        return caps\n",
        "\n",
        "    @traceable\n",
        "    async def use_mcp_tool(self, tool_name: str, tool_params: Dict) -> Dict:\n",
        "        \"\"\"Use MCP tools with built-in observability and A2A notifications.\"\"\"\n",
        "        if tool_name in self.mcp_server.tools:\n",
        "            self.performance_metrics[\"mcp_tools_used\"] += 1\n",
        "            result = await self.mcp_server.tools[tool_name](**tool_params)\n",
        "\n",
        "            # Notify other agents about tool usage\n",
        "            await self.a2a.broadcast_message(self.agent_id, {\n",
        "                \"type\": \"mcp_tool_used\", \"tool\": tool_name,\n",
        "                \"summary\": str(result)[:100] + \"...\"\n",
        "            })\n",
        "            return result\n",
        "        return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "    def get_enhanced_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive agent status with observability metrics.\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"a2a_registered\": self.agent_id in self.a2a.agent_registries,\n",
        "            \"health_status\": \"optimal\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 8: Enhanced Specialized Agents\n",
        "class EnhancedPricingAgent(EnhancedRetailAgent):\n",
        "    \"\"\"A specialized agent for pricing strategy, using all system components.\"\"\"\n",
        "    def __init__(self, agent_id: str):\n",
        "        super().__init__(agent_id, \"enhanced_pricing_strategist\", \"gpt-4\")\n",
        "\n",
        "    @traceable\n",
        "    async def analyze_pricing(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"Enhanced pricing analysis workflow.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        try:\n",
        "            # 1. Get context from Vector DB\n",
        "            self.performance_metrics[\"vector_db_queries\"] += 1\n",
        "            context = await self.vector_db.enhanced_search(\n",
        "                f\"pricing strategy for {product_data.get('category')}\",\n",
        "                category=\"pricing\", agent_id=self.agent_id\n",
        "            )\n",
        "\n",
        "            # 2. Use MCP tools for advanced analysis\n",
        "            mcp_analysis = await self.use_mcp_tool(\"price_optimizer\", {\n",
        "                \"product_data\": product_data,\n",
        "                \"market_conditions\": {**market_conditions, \"vector_context\": context}\n",
        "            })\n",
        "\n",
        "            # 3. Generate final recommendation\n",
        "            final_recommendation = await self._generate_final_recommendation(\n",
        "                product_data, mcp_analysis, context\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": final_recommendation,\n",
        "                \"mcp_analysis_used\": mcp_analysis,\n",
        "                \"context_used\": context,\n",
        "                \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    async def _generate_final_recommendation(self, product_data: Dict, mcp_analysis: Dict, context: List) -> str:\n",
        "        \"\"\"Synthesize a final recommendation using an LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Synthesize a final pricing recommendation based on the following data:\n",
        "        PRODUCT: {product_data}\n",
        "        MCP ANALYSIS: {mcp_analysis}\n",
        "        VECTOR DB CONTEXT: {context}\n",
        "\n",
        "        Provide a comprehensive pricing strategy for executive decision-making.\n",
        "        \"\"\"\n",
        "        response = await self.llm.ainvoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "\n",
        "# Step 9: Enhanced Orchestrator with Full Observability\n",
        "class ObservableRetailOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the agent army, runs workflows, and provides a centralized\n",
        "    view of system-wide observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.workflow_history = []\n",
        "        self.observability_metrics = {\n",
        "            \"total_workflows\": 0, \"successful_workflows\": 0, \"average_workflow_time\": 0.0\n",
        "        }\n",
        "        self.setup_enhanced_agent_army()\n",
        "\n",
        "    @traceable\n",
        "    def setup_enhanced_agent_army(self):\n",
        "        \"\"\"Initialize the enhanced agent army.\"\"\"\n",
        "        self.agents = {\n",
        "            \"pricing_alpha_enhanced\": EnhancedPricingAgent(\"pricing_alpha_enhanced\"),\n",
        "            \"pricing_beta_enhanced\": EnhancedPricingAgent(\"pricing_beta_enhanced\"),\n",
        "        }\n",
        "        print(f\"‚úÖ Enhanced Agent Army Initialized: {len(self.agents)} observable agents\")\n",
        "\n",
        "    @traceable\n",
        "    async def run_observable_pricing_workflow(self, product_data: Dict) -> Dict:\n",
        "        \"\"\"Run a pricing workflow with full observability.\"\"\"\n",
        "        workflow_start = datetime.now()\n",
        "        self.observability_metrics[\"total_workflows\"] += 1\n",
        "        try:\n",
        "            print(f\"üöÄ Starting OBSERVABLE pricing workflow for {product_data.get('name')}...\")\n",
        "            market_context = await observable_vector_db.enhanced_search(f\"market analysis for {product_data.get('category')}\", k=3)\n",
        "\n",
        "            pricing_agents = [agent for aid, agent in self.agents.items() if \"pricing\" in aid]\n",
        "            tasks = [agent.analyze_pricing(product_data, {\"context\": market_context}) for agent in pricing_agents]\n",
        "            pricing_results = await asyncio.gather(*tasks)\n",
        "\n",
        "            aggregated_recommendation = await self.aggregate_enhanced_recommendations(pricing_results, product_data)\n",
        "\n",
        "            workflow_time = (datetime.now() - workflow_start).total_seconds()\n",
        "            self.observability_metrics[\"successful_workflows\"] += 1\n",
        "            total_workflows = self.observability_metrics[\"successful_workflows\"]\n",
        "            self.observability_metrics[\"average_workflow_time\"] = ((self.observability_metrics[\"average_workflow_time\"] * (total_workflows-1)) + workflow_time) / total_workflows\n",
        "\n",
        "            workflow_result = {\n",
        "                \"workflow_type\": \"observable_pricing\",\n",
        "                \"aggregated_recommendation\": aggregated_recommendation,\n",
        "                \"workflow_duration\": workflow_time\n",
        "            }\n",
        "            self.workflow_history.append(workflow_result)\n",
        "            print(f\"‚úÖ Observable pricing workflow completed in {workflow_time:.2f}s\")\n",
        "            return workflow_result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Observable workflow error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @traceable\n",
        "    async def aggregate_enhanced_recommendations(self, results: List[Dict], context_data: Dict) -> Dict:\n",
        "        \"\"\"Aggregate recommendations from multiple agents using an LLM.\"\"\"\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "        prompt = f\"\"\"\n",
        "        Aggregate these pricing recommendations from multiple AI agents:\n",
        "        CONTEXT: {context_data}\n",
        "        AGENT ANALYSES: {json.dumps(results, indent=2)}\n",
        "        Provide a single, consolidated strategy.\n",
        "        \"\"\"\n",
        "        response = await llm.ainvoke(prompt)\n",
        "        return {\"consolidated_strategy\": response.content}\n",
        "\n",
        "    def get_comprehensive_observability(self) -> Dict:\n",
        "        \"\"\"Get comprehensive observability data from all system components.\"\"\"\n",
        "        agent_statuses = {aid: agent.get_enhanced_status() for aid, agent in self.agents.items()}\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"orchestrator_metrics\": self.observability_metrics,\n",
        "            \"agent_army_status\": {\n",
        "                \"total_agents\": len(self.agents),\n",
        "                \"agent_breakdown\": {\"pricing_agents\": len([a for a in self.agents if \"pricing\" in a])}\n",
        "            },\n",
        "            \"a2a_communication_metrics\": a2a_communication.get_communication_metrics(),\n",
        "            \"vector_db_performance\": observable_vector_db.get_performance_metrics(),\n",
        "            \"workflow_history_summary\": {\n",
        "                \"total_workflows\": len(self.workflow_history),\n",
        "                \"success_rate\": self.observability_metrics[\"successful_workflows\"] / max(1, self.observability_metrics[\"total_workflows\"])\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 10: Generate Sample Data\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data for the demonstration.\"\"\"\n",
        "    return {\n",
        "        \"products\": [\n",
        "            {\"product_id\": \"PROD_001\", \"name\": \"Wireless Bluetooth Headphones\", \"category\": \"electronics\", \"cost_price\": 45.0, \"current_price\": 99.99},\n",
        "            {\"product_id\": \"PROD_002\", \"name\": \"Smartphone Protective Case\", \"category\": \"accessories\", \"cost_price\": 4.50, \"current_price\": 19.99}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"üìä Generating sample data...\")\n",
        "sample_data = generate_sample_data()\n",
        "\n",
        "\n",
        "# Step 11: Initialize Enhanced Orchestrator\n",
        "print(\"üéØ Initializing Observable Retail Agent Army Orchestrator...\")\n",
        "observable_orchestrator = ObservableRetailOrchestrator()\n",
        "\n",
        "\n",
        "# Step 12: Enhanced Demo with Full Observability\n",
        "async def run_observable_demo():\n",
        "    \"\"\"Run the main demo workflow and print observability metrics.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run pricing workflows\n",
        "    for product in sample_data['products']:\n",
        "        await observable_orchestrator.run_observable_pricing_workflow(product)\n",
        "\n",
        "    # Show final dashboard data\n",
        "    final_observability = observable_orchestrator.get_comprehensive_observability()\n",
        "    print(\"\\n3. üìà FINAL OBSERVABILITY DASHBOARD DATA\")\n",
        "    print(json.dumps(final_observability, indent=2, default=str))\n",
        "\n",
        "    return final_observability\n",
        "\n",
        "\n",
        "# Step 13: Main execution block\n",
        "async def main():\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
        "        print(\"\\n\\n‚ùå Cannot run demo. Please set your OpenAI API key at the top of the script.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüöÄ Starting enhanced observable demo...\")\n",
        "    final_observability = await run_observable_demo()\n",
        "\n",
        "    # Step 14: Visualization is better in a notebook environment\n",
        "    # Here we print the key insights\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéä DEMO COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîç OBSERVABILITY FEATURES DEMONSTRATED:\")\n",
        "    print(\"  ‚úÖ MCP (Model Context Protocol) Integration\")\n",
        "    print(\"  ‚úÖ A2A (Agent-to-Agent) Communication Layer\")\n",
        "    print(\"  ‚úÖ LangSmith AI Observability & Tracing\")\n",
        "    print(\"  ‚úÖ Real-time Performance Monitoring\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In a script, you run the asyncio event loop.\n",
        "    # In a Jupyter notebook, you can often just use `await main()` in a cell.\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Installing required packages...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command '['/usr/bin/python3', '-m', 'pip', 'install', 'langchain', 'langchain-openai', 'langgraph', 'langchain-community', 'chromadb', 'sentence-transformers', 'lightgbm', 'pandas', 'numpy', 'matplotlib', 'scikit-learn', 'asyncio', 'aiohttp', 'langsmith', 'mcp-client', 'guardrails-ai']' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3932704854.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightgbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangsmith\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguardrails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ All packages already installed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3932704854.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"install\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"langchain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"langchain-openai\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"langgraph\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"langchain-community\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chromadb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence-transformers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lightgbm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matplotlib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scikit-learn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"asyncio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aiohttp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"langsmith\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mcp-client\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"guardrails-ai\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ All packages installed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/python3', '-m', 'pip', 'install', 'langchain', 'langchain-openai', 'langgraph', 'langchain-community', 'chromadb', 'sentence-transformers', 'lightgbm', 'pandas', 'numpy', 'matplotlib', 'scikit-learn', 'asyncio', 'aiohttp', 'langsmith', 'mcp-client', 'guardrails-ai']' returned non-zero exit status 1."
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "g3lWjg85Ivkb",
        "outputId": "00d97956-24cd-40b2-d703-5fd44d7a3985"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "# Note: In a local environment, you would run these commands in your terminal.\n",
        "# In a Jupyter/Colab notebook, the '!' prefix executes them as shell commands.\n",
        "try:\n",
        "    import langchain, chromadb, sentence_transformers, lightgbm, pandas, numpy, matplotlib, sklearn, asyncio, aiohttp, langsmith, mcp, guardrails\n",
        "    print(\"‚úÖ All packages already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required packages...\")\n",
        "    # Use !pip install for Colab environment\n",
        "    !pip install langchain langchain-openai langgraph langchain-community chromadb sentence-transformers lightgbm pandas numpy matplotlib scikit-learn asyncio aiohttp langsmith mcp-client guardrails-ai\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# LangSmith for AI Observability\n",
        "import langsmith\n",
        "from langsmith import Client, traceable, run_trees\n",
        "\n",
        "# MCP (Model Context Protocol) imports\n",
        "# This is kept for conceptual context, though no specific client functions are called.\n",
        "import mcp\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 3: Configure API Keys and Observability\n",
        "print(\"üîë Setting up API Keys and Observability...\")\n",
        "\n",
        "# It's highly recommended to use environment variables or a secrets management tool.\n",
        "# For this example, we'll use placeholders.\n",
        "# IMPORTANT: Replace these with your actual keys.\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', \"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\")\n",
        "LANGSMITH_API_KEY = os.environ.get('LANGSMITH_API_KEY', \"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\")\n",
        "LANGSMITH_PROJECT = os.environ.get('LANGSMITH_PROJECT', \"retail-agent-pool\")\n",
        "\n",
        "# Set environment variables for LangChain and LangSmith\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # Enables LangSmith tracing\n",
        "\n",
        "# Initialize LangSmith client\n",
        "try:\n",
        "    if LANGSMITH_API_KEY != \"YOUR_LANGSMITH_API_KEY\":\n",
        "        langsmith_client = Client()\n",
        "        print(\"‚úÖ LangSmith client initialized successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è LangSmith API key not set. Skipping client initialization.\")\n",
        "        langsmith_client = None\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è LangSmith initialization warning: {e}\")\n",
        "    langsmith_client = None\n",
        "\n",
        "# Test the OpenAI API key\n",
        "try:\n",
        "    if OPENAI_API_KEY != \"YOUR_OPENAI_API_KEY\":\n",
        "        test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        test_response = test_llm.invoke(\"Test\")\n",
        "        print(\"‚úÖ OpenAI API key verified successfully!\")\n",
        "    else:\n",
        "         print(\"‚ùå OpenAI API key is not set. Please replace 'YOUR_OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API key error: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: MCP Server Implementation for Retail\n",
        "class RetailMCPServer:\n",
        "    \"\"\"\n",
        "    A simulated Model Context Protocol (MCP) server that provides specialized tools\n",
        "    for retail analysis. Each tool is traceable for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"price_optimizer\": self.price_optimization_tool,\n",
        "            \"demand_forecaster\": self.demand_forecasting_tool,\n",
        "            \"inventory_optimizer\": self.inventory_optimization_tool,\n",
        "            \"customer_analyzer\": self.customer_analytics_tool,\n",
        "            \"competitor_monitor\": self.competitive_intelligence_tool,\n",
        "            \"market_analyzer\": self.market_analysis_tool\n",
        "        }\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "\n",
        "    @traceable\n",
        "    async def price_optimization_tool(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced price optimization using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Perform advanced price optimization analysis:\n",
        "\n",
        "            PRODUCT: {product_data}\n",
        "            MARKET: {market_conditions}\n",
        "\n",
        "            Calculate:\n",
        "            1. Price elasticity estimation\n",
        "            2. Optimal price point with confidence intervals\n",
        "            3. Competitive positioning strategy\n",
        "            4. Revenue maximization approach\n",
        "            5. Risk assessment\n",
        "\n",
        "            Return structured JSON analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"price_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": response.content,\n",
        "                \"recommendations\": {\n",
        "                    \"optimal_price_range\": {\"min\": 45.0, \"max\": 125.0, \"recommended\": 89.99},\n",
        "                    \"confidence_score\": 0.87,\n",
        "                    \"expected_revenue_impact\": \"+15-25%\",\n",
        "                    \"risk_level\": \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"price_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def demand_forecasting_tool(self, historical_data: List, market_signals: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Demand forecasting with market signals.\"\"\"\n",
        "        try:\n",
        "            forecast_data = {\n",
        "                \"next_30_days\": np.random.normal(100, 20, 30).tolist(),\n",
        "                \"confidence_interval\": {\"lower\": 80, \"upper\": 120},\n",
        "                \"seasonality_factor\": 1.15,\n",
        "                \"trend_direction\": \"upward\"\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"demand_forecaster\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"forecast\": forecast_data,\n",
        "                \"market_signals_used\": list(market_signals.keys()),\n",
        "                \"data_points_analyzed\": len(historical_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"demand_forecaster\"}\n",
        "\n",
        "    @traceable\n",
        "    async def inventory_optimization_tool(self, current_stock: Dict, demand_forecast: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Inventory optimization based on demand forecasting.\"\"\"\n",
        "        try:\n",
        "            optimization_recommendations = {\n",
        "                \"safety_stock_levels\": {\n",
        "                    \"high_demand\": current_stock.get('current_stock', 0) * 0.3,\n",
        "                    \"medium_demand\": current_stock.get('current_stock', 0) * 0.2,\n",
        "                    \"low_demand\": current_stock.get('current_stock', 0) * 0.1\n",
        "                },\n",
        "                \"reorder_points\": {\n",
        "                    \"urgent\": current_stock.get('reorder_point', 0) * 0.8,\n",
        "                    \"standard\": current_stock.get('reorder_point', 0),\n",
        "                    \"relaxed\": current_stock.get('reorder_point', 0) * 1.2\n",
        "                },\n",
        "                \"cost_optimization\": {\n",
        "                    \"expected_savings\": \"15-20%\",\n",
        "                    \"stockout_risk_reduction\": \"40-60%\",\n",
        "                    \"carrying_cost_optimization\": \"10-15%\"\n",
        "                }\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"inventory_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"optimization\": optimization_recommendations,\n",
        "                \"demand_forecast_incorporated\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"inventory_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def customer_analytics_tool(self, customer_data: Dict, purchase_history: List) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced customer analytics using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze customer behavior and generate insights:\n",
        "\n",
        "            CUSTOMER: {customer_data}\n",
        "            PURCHASE HISTORY: {purchase_history[:5]}\n",
        "\n",
        "            Provide:\n",
        "            1. Customer lifetime value prediction\n",
        "            2. Churn risk assessment\n",
        "            3. Personalization opportunities\n",
        "            4. Retention strategy\n",
        "            5. Cross-selling recommendations\n",
        "\n",
        "            Return structured analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"customer_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"customer_insights\": response.content,\n",
        "                \"metrics\": {\n",
        "                    \"lifetime_value_score\": self._calculate_ltv(purchase_history),\n",
        "                    \"churn_risk\": \"low\" if len(purchase_history) > 3 else \"medium\",\n",
        "                    \"engagement_level\": \"high\" if len(purchase_history) > 5 else \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"customer_analyzer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def competitive_intelligence_tool(self, product_data: Dict, market_scope: str = \"local\") -> Dict:\n",
        "        \"\"\"MCP Tool: Competitive market intelligence.\"\"\"\n",
        "        try:\n",
        "            competitor_analysis = {\n",
        "                \"main_competitors\": [\"Competitor A\", \"Competitor B\", \"Competitor C\"],\n",
        "                \"price_comparison\": {\n",
        "                    \"our_price\": product_data.get('current_price', 0),\n",
        "                    \"competitor_avg\": product_data.get('current_price', 0) * 0.9,\n",
        "                    \"market_range\": {\n",
        "                        \"min\": product_data.get('current_price', 0) * 0.7,\n",
        "                        \"max\": product_data.get('current_price', 0) * 1.3\n",
        "                    }\n",
        "                },\n",
        "                \"competitive_positioning\": \"market_leader\" if product_data.get('current_price', 0) > 100 else \"value_player\",\n",
        "                \"recommendations\": [\n",
        "                    \"Monitor competitor pricing weekly\",\n",
        "                    \"Differentiate through value-added services\",\n",
        "                    \"Consider bundle pricing strategies\"\n",
        "                ]\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"competitor_monitor\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": competitor_analysis,\n",
        "                \"market_scope\": market_scope\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"competitor_monitor\"}\n",
        "\n",
        "    @traceable\n",
        "    async def market_analysis_tool(self, product_category: str, timeframe: str = \"30d\") -> Dict:\n",
        "        \"\"\"MCP Tool: Comprehensive market analysis using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze market conditions for {product_category} over {timeframe}:\n",
        "\n",
        "            Provide:\n",
        "            1. Market trends and growth projections\n",
        "            2. Consumer behavior insights\n",
        "            3. Regulatory considerations\n",
        "            4. Technology impacts\n",
        "            5. Strategic recommendations\n",
        "\n",
        "            Focus on actionable insights for retail pricing and inventory.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"market_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"market_analysis\": response.content,\n",
        "                \"timeframe\": timeframe,\n",
        "                \"category\": product_category\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"market_analyzer\"}\n",
        "\n",
        "    def _calculate_ltv(self, purchase_history: List) -> float:\n",
        "        \"\"\"Helper function to calculate customer lifetime value.\"\"\"\n",
        "        if not purchase_history:\n",
        "            return 0.0\n",
        "        total_revenue = sum(p.get('revenue', 0) for p in purchase_history)\n",
        "        purchase_count = len(purchase_history)\n",
        "        avg_order_value = total_revenue / purchase_count if purchase_count > 0 else 0\n",
        "        return min(round(avg_order_value * purchase_count * 0.1, 2), 1000.0)\n",
        "\n",
        "print(\"üõ†Ô∏è Initializing MCP Server...\")\n",
        "mcp_server = RetailMCPServer()\n",
        "\n",
        "\n",
        "# Step 5: A2A (Agent-to-Agent) Communication Layer\n",
        "class A2ACommunicationLayer:\n",
        "    \"\"\"\n",
        "    Manages communication between agents, including direct messages, broadcasts,\n",
        "    and consensus requests, with full tracing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agent_registries = {}\n",
        "        self.conversation_history = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    @traceable\n",
        "    async def register_agent(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        \"\"\"Register an agent in the A2A network.\"\"\"\n",
        "        self.agent_registries[agent_id] = {\n",
        "            \"agent_type\": agent_type,\n",
        "            \"capabilities\": capabilities,\n",
        "            \"status\": \"active\",\n",
        "            \"last_heartbeat\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.performance_metrics[agent_id] = {\n",
        "            \"messages_sent\": 0, \"messages_received\": 0,\n",
        "            \"response_time_avg\": 0.0, \"error_rate\": 0.0\n",
        "        }\n",
        "        print(f\"‚úÖ Agent registered: {agent_id} ({agent_type})\")\n",
        "\n",
        "    @traceable\n",
        "    async def send_message(self, from_agent: str, to_agent: str, message: Dict, message_type: str = \"direct\"):\n",
        "        \"\"\"Send a message between agents with tracing.\"\"\"\n",
        "        try:\n",
        "            message_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.now().isoformat()\n",
        "            run_tree = langsmith.get_run_tree()\n",
        "            trace_id = run_tree.id if run_tree else 'unknown'\n",
        "\n",
        "            message_package = {\n",
        "                \"message_id\": message_id, \"from_agent\": from_agent,\n",
        "                \"to_agent\": to_agent, \"message_type\": message_type,\n",
        "                \"content\": message, \"timestamp\": timestamp, \"trace_id\": trace_id\n",
        "            }\n",
        "\n",
        "            if from_agent not in self.conversation_history:\n",
        "                self.conversation_history[from_agent] = []\n",
        "            self.conversation_history[from_agent].append(message_package)\n",
        "\n",
        "            self.performance_metrics[from_agent][\"messages_sent\"] += 1\n",
        "            if to_agent in self.performance_metrics:\n",
        "                self.performance_metrics[to_agent][\"messages_received\"] += 1\n",
        "\n",
        "            print(f\"üì® A2A Message: {from_agent} ‚Üí {to_agent} ({message_type})\")\n",
        "            return message_id\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Message Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @traceable\n",
        "    async def broadcast_message(self, from_agent: str, message: Dict, agent_filter: List[str] = None):\n",
        "        \"\"\"Broadcast a message to multiple agents.\"\"\"\n",
        "        try:\n",
        "            target_agents = agent_filter if agent_filter else list(self.agent_registries.keys())\n",
        "            target_agents = [agent for agent in target_agents if agent != from_agent]\n",
        "\n",
        "            tasks = [self.send_message(from_agent, agent_id, message, \"broadcast\") for agent_id in target_agents]\n",
        "            message_ids = await asyncio.gather(*tasks)\n",
        "            return message_ids\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Broadcast Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_communication_metrics(self) -> Dict:\n",
        "        \"\"\"Get A2A communication performance metrics.\"\"\"\n",
        "        total_messages = sum(m[\"messages_sent\"] for m in self.performance_metrics.values())\n",
        "        active_agents = len([a for a, i in self.agent_registries.items() if i[\"status\"] == \"active\"])\n",
        "        return {\n",
        "            \"total_agents_registered\": len(self.agent_registries),\n",
        "            \"active_agents\": active_agents,\n",
        "            \"total_messages_exchanged\": total_messages,\n",
        "            \"agent_performance\": self.performance_metrics,\n",
        "            \"system_health\": \"optimal\" if active_agents > 0 else \"degraded\"\n",
        "        }\n",
        "\n",
        "print(\"üîÑ Initializing A2A Communication Layer...\")\n",
        "a2a_communication = A2ACommunicationLayer()\n",
        "\n",
        "\n",
        "# Step 6: Enhanced Vector Database with Observability\n",
        "class ObservableVectorDB:\n",
        "    \"\"\"\n",
        "    A wrapper for a Chroma vector database that adds observability and performance tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_store = None\n",
        "        self.query_metrics = {\"total_queries\": 0, \"successful_queries\": 0, \"average_response_time\": 0.0}\n",
        "        self.setup_sample_data()\n",
        "\n",
        "    @traceable\n",
        "    def setup_sample_data(self):\n",
        "        \"\"\"Initialize with comprehensive retail sample data.\"\"\"\n",
        "        sample_docs = [\n",
        "            Document(page_content=\"Premium pricing strategy works best for electronics with 40-60% markup\", metadata={\"category\": \"pricing\", \"product_type\": \"electronics\"}),\n",
        "            Document(page_content=\"Competitive pricing essential for smartphone accessories market\", metadata={\"category\": \"pricing\", \"product_type\": \"accessories\"}),\n",
        "            Document(page_content=\"Safety stock levels should cover 2-week demand for fast-moving goods\", metadata={\"category\": \"inventory\", \"metric\": \"safety_stock\"}),\n",
        "            Document(page_content=\"Holiday season demand increases 60-80% for electronics\", metadata={\"category\": \"trends\", \"season\": \"Q4\"}),\n",
        "            Document(page_content=\"Price sensitivity is highest in the budget segment\", metadata={\"category\": \"customer\", \"insight\": \"pricing\"})\n",
        "        ]\n",
        "        self.vector_store = Chroma.from_documents(documents=sample_docs, embedding=self.embeddings, persist_directory=\"./chroma_retail_db\")\n",
        "        print(\"‚úÖ Observable Vector database initialized\")\n",
        "\n",
        "    @traceable\n",
        "    async def enhanced_search(self, query: str, category: str = None, k: int = 3, agent_id: str = \"unknown\") -> List[Dict]:\n",
        "        \"\"\"Enhanced semantic search with observability.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.query_metrics[\"total_queries\"] += 1\n",
        "        try:\n",
        "            filter_dict = {\"category\": category} if category else {}\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.query_metrics[\"successful_queries\"] += 1\n",
        "            total_queries = self.query_metrics[\"successful_queries\"]\n",
        "            self.query_metrics[\"average_response_time\"] = ((self.query_metrics[\"average_response_time\"] * (total_queries - 1)) + response_time) / total_queries\n",
        "\n",
        "            formatted_results = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"similarity_score\": float(score)} for doc, score in results]\n",
        "            print(f\"üîç VectorDB Search: '{query}' ‚Üí {len(formatted_results)} results\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå VectorDB Search Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Get vector database performance metrics.\"\"\"\n",
        "        success_rate = self.query_metrics[\"successful_queries\"] / max(1, self.query_metrics[\"total_queries\"])\n",
        "        return {**self.query_metrics, \"success_rate\": success_rate, \"health_status\": \"optimal\"}\n",
        "\n",
        "print(\"üõ¢Ô∏è Initializing Observable Vector Database...\")\n",
        "observable_vector_db = ObservableVectorDB()\n",
        "\n",
        "\n",
        "# Step 7: Enhanced Base Agent with MCP and A2A Integration\n",
        "class EnhancedRetailAgent:\n",
        "    \"\"\"\n",
        "    A base class for retail AI agents, integrating MCP tools, A2A communication,\n",
        "    and an observable vector database.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str, agent_type: str, model: str = \"gpt-4\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = ChatOpenAI(model=model, temperature=0.1)\n",
        "        self.mcp_server = mcp_server\n",
        "        self.a2a = a2a_communication\n",
        "        self.vector_db = observable_vector_db\n",
        "        self.history = []\n",
        "        self.performance_metrics = {\n",
        "            \"tasks_completed\": 0, \"mcp_tools_used\": 0,\n",
        "            \"a2a_messages_sent\": 0, \"vector_db_queries\": 0,\n",
        "            \"avg_response_time\": 0.0, \"success_rate\": 1.0\n",
        "        }\n",
        "        asyncio.create_task(self._register_with_a2a())\n",
        "\n",
        "    async def _register_with_a2a(self):\n",
        "        \"\"\"Register agent with the A2A communication layer.\"\"\"\n",
        "        await self.a2a.register_agent(self.agent_id, self.agent_type, self._get_agent_capabilities())\n",
        "\n",
        "    def _get_agent_capabilities(self) -> List[str]:\n",
        "        \"\"\"Define agent capabilities for A2A registration.\"\"\"\n",
        "        caps = [\"analysis\", \"decision_support\", \"collaboration\"]\n",
        "        if \"pricing\" in self.agent_type: caps.extend([\"price_optimization\", \"competitive_analysis\"])\n",
        "        elif \"inventory\" in self.agent_type: caps.extend([\"demand_forecasting\", \"stock_optimization\"])\n",
        "        return caps\n",
        "\n",
        "    @traceable\n",
        "    async def use_mcp_tool(self, tool_name: str, tool_params: Dict) -> Dict:\n",
        "        \"\"\"Use MCP tools with built-in observability and A2A notifications.\"\"\"\n",
        "        if tool_name in self.mcp_server.tools:\n",
        "            self.performance_metrics[\"mcp_tools_used\"] += 1\n",
        "            result = await self.mcp_server.tools[tool_name](**tool_params)\n",
        "\n",
        "            # Notify other agents about tool usage\n",
        "            await self.a2a.broadcast_message(self.agent_id, {\n",
        "                \"type\": \"mcp_tool_used\", \"tool\": tool_name,\n",
        "                \"summary\": str(result)[:100] + \"...\"\n",
        "            })\n",
        "            return result\n",
        "        return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "    def get_enhanced_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive agent status with observability metrics.\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"a2a_registered\": self.agent_id in self.a2a.agent_registries,\n",
        "            \"health_status\": \"optimal\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 8: Enhanced Specialized Agents\n",
        "class EnhancedPricingAgent(EnhancedRetailAgent):\n",
        "    \"\"\"A specialized agent for pricing strategy, using all system components.\"\"\"\n",
        "    def __init__(self, agent_id: str):\n",
        "        super().__init__(agent_id, \"enhanced_pricing_strategist\", \"gpt-4\")\n",
        "\n",
        "    @traceable\n",
        "    async def analyze_pricing(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"Enhanced pricing analysis workflow.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        try:\n",
        "            # 1. Get context from Vector DB\n",
        "            self.performance_metrics[\"vector_db_queries\"] += 1\n",
        "            context = await self.vector_db.enhanced_search(\n",
        "                f\"pricing strategy for {product_data.get('category')}\",\n",
        "                category=\"pricing\", agent_id=self.agent_id\n",
        "            )\n",
        "\n",
        "            # 2. Use MCP tools for advanced analysis\n",
        "            mcp_analysis = await self.use_mcp_tool(\"price_optimizer\", {\n",
        "                \"product_data\": product_data,\n",
        "                \"market_conditions\": {**market_conditions, \"vector_context\": context}\n",
        "            })\n",
        "\n",
        "            # 3. Generate final recommendation\n",
        "            final_recommendation = await self._generate_final_recommendation(\n",
        "                product_data, mcp_analysis, context\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": final_recommendation,\n",
        "                \"mcp_analysis_used\": mcp_analysis,\n",
        "                \"context_used\": context,\n",
        "                \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    async def _generate_final_recommendation(self, product_data: Dict, mcp_analysis: Dict, context: List) -> str:\n",
        "        \"\"\"Synthesize a final recommendation using an LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Synthesize a final pricing recommendation based on the following data:\n",
        "        PRODUCT: {product_data}\n",
        "        MCP ANALYSIS: {mcp_analysis}\n",
        "        VECTOR DB CONTEXT: {context}\n",
        "\n",
        "        Provide a comprehensive pricing strategy for executive decision-making.\n",
        "        \"\"\"\n",
        "        response = await self.llm.ainvoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "\n",
        "# Step 9: Enhanced Orchestrator with Full Observability\n",
        "class ObservableRetailOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the agent army, runs workflows, and provides a centralized\n",
        "    view of system-wide observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.workflow_history = []\n",
        "        self.observability_metrics = {\n",
        "            \"total_workflows\": 0, \"successful_workflows\": 0, \"average_workflow_time\": 0.0\n",
        "        }\n",
        "        self.setup_enhanced_agent_army()\n",
        "\n",
        "    @traceable\n",
        "    def setup_enhanced_agent_army(self):\n",
        "        \"\"\"Initialize the enhanced agent army.\"\"\"\n",
        "        self.agents = {\n",
        "            \"pricing_alpha_enhanced\": EnhancedPricingAgent(\"pricing_alpha_enhanced\"),\n",
        "            \"pricing_beta_enhanced\": EnhancedPricingAgent(\"pricing_beta_enhanced\"),\n",
        "        }\n",
        "        print(f\"‚úÖ Enhanced Agent Army Initialized: {len(self.agents)} observable agents\")\n",
        "\n",
        "    @traceable\n",
        "    async def run_observable_pricing_workflow(self, product_data: Dict) -> Dict:\n",
        "        \"\"\"Run a pricing workflow with full observability.\"\"\"\n",
        "        workflow_start = datetime.now()\n",
        "        self.observability_metrics[\"total_workflows\"] += 1\n",
        "        try:\n",
        "            print(f\"üöÄ Starting OBSERVABLE pricing workflow for {product_data.get('name')}...\")\n",
        "            market_context = await observable_vector_db.enhanced_search(f\"market analysis for {product_data.get('category')}\", k=3)\n",
        "\n",
        "            pricing_agents = [agent for aid, agent in self.agents.items() if \"pricing\" in aid]\n",
        "            tasks = [agent.analyze_pricing(product_data, {\"context\": market_context}) for agent in pricing_agents]\n",
        "            pricing_results = await asyncio.gather(*tasks)\n",
        "\n",
        "            aggregated_recommendation = await self.aggregate_enhanced_recommendations(pricing_results, product_data)\n",
        "\n",
        "            workflow_time = (datetime.now() - workflow_start).total_seconds()\n",
        "            self.observability_metrics[\"successful_workflows\"] += 1\n",
        "            total_workflows = self.observability_metrics[\"successful_workflows\"]\n",
        "            self.observability_metrics[\"average_workflow_time\"] = ((self.observability_metrics[\"average_workflow_time\"] * (total_workflows-1)) + workflow_time) / total_workflows\n",
        "\n",
        "            workflow_result = {\n",
        "                \"workflow_type\": \"observable_pricing\",\n",
        "                \"aggregated_recommendation\": aggregated_recommendation,\n",
        "                \"workflow_duration\": workflow_time\n",
        "            }\n",
        "            self.workflow_history.append(workflow_result)\n",
        "            print(f\"‚úÖ Observable pricing workflow completed in {workflow_time:.2f}s\")\n",
        "            return workflow_result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Observable workflow error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @traceable\n",
        "    async def aggregate_enhanced_recommendations(self, results: List[Dict], context_data: Dict) -> Dict:\n",
        "        \"\"\"Aggregate recommendations from multiple agents using an LLM.\"\"\"\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "        prompt = f\"\"\"\n",
        "        Aggregate these pricing recommendations from multiple AI agents:\n",
        "        CONTEXT: {context_data}\n",
        "        AGENT ANALYSES: {json.dumps(results, indent=2)}\n",
        "        Provide a single, consolidated strategy.\n",
        "        \"\"\"\n",
        "        response = await llm.ainvoke(prompt)\n",
        "        return {\"consolidated_strategy\": response.content}\n",
        "\n",
        "    def get_comprehensive_observability(self) -> Dict:\n",
        "        \"\"\"Get comprehensive observability data from all system components.\"\"\"\n",
        "        agent_statuses = {aid: agent.get_enhanced_status() for aid, agent in self.agents.items()}\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"orchestrator_metrics\": self.observability_metrics,\n",
        "            \"agent_army_status\": {\n",
        "                \"total_agents\": len(self.agents),\n",
        "                \"agent_breakdown\": {\"pricing_agents\": len([a for a in self.agents if \"pricing\" in a])}\n",
        "            },\n",
        "            \"a2a_communication_metrics\": a2a_communication.get_communication_metrics(),\n",
        "            \"vector_db_performance\": observable_vector_db.get_performance_metrics(),\n",
        "            \"workflow_history_summary\": {\n",
        "                \"total_workflows\": len(self.workflow_history),\n",
        "                \"success_rate\": self.observability_metrics[\"successful_workflows\"] / max(1, self.observability_metrics[\"total_workflows\"])\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 10: Generate Sample Data\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data for the demonstration.\"\"\"\n",
        "    return {\n",
        "        \"products\": [\n",
        "            {\"product_id\": \"PROD_001\", \"name\": \"Wireless Bluetooth Headphones\", \"category\": \"electronics\", \"cost_price\": 45.0, \"current_price\": 99.99},\n",
        "            {\"product_id\": \"PROD_002\", \"name\": \"Smartphone Protective Case\", \"category\": \"accessories\", \"cost_price\": 4.50, \"current_price\": 19.99}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"üìä Generating sample data...\")\n",
        "sample_data = generate_sample_data()\n",
        "\n",
        "\n",
        "# Step 11: Initialize Enhanced Orchestrator\n",
        "print(\"üéØ Initializing Observable Retail Agent Army Orchestrator...\")\n",
        "observable_orchestrator = ObservableRetailOrchestrator()\n",
        "\n",
        "\n",
        "# Step 12: Enhanced Demo with Full Observability\n",
        "async def run_observable_demo():\n",
        "    \"\"\"Run the main demo workflow and print observability metrics.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run pricing workflows\n",
        "    for product in sample_data['products']:\n",
        "        await observable_orchestrator.run_observable_pricing_workflow(product)\n",
        "\n",
        "    # Show final dashboard data\n",
        "    final_observability = observable_orchestrator.get_comprehensive_observability()\n",
        "    print(\"\\n3. üìà FINAL OBSERVABILITY DASHBOARD DATA\")\n",
        "    print(json.dumps(final_observability, indent=2, default=str))\n",
        "\n",
        "    return final_observability\n",
        "\n",
        "\n",
        "# Step 13: Main execution block\n",
        "async def main():\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
        "        print(\"\\n\\n‚ùå Cannot run demo. Please set your OpenAI API key at the top of the script.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüöÄ Starting enhanced observable demo...\")\n",
        "    final_observability = await run_observable_demo()\n",
        "\n",
        "    # Step 14: Visualization is better in a notebook environment\n",
        "    # Here we print the key insights\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéä DEMO COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîç OBSERVABILITY FEATURES DEMONSTRATED:\")\n",
        "    print(\"  ‚úÖ MCP (Model Context Protocol) Integration\")\n",
        "    print(\"  ‚úÖ A2A (Agent-to-Agent) Communication Layer\")\n",
        "    print(\"  ‚úÖ LangSmith AI Observability & Tracing\")\n",
        "    print(\"  ‚úÖ Real-time Performance Monitoring\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In a script, you run the asyncio event loop.\n",
        "    # In a Jupyter notebook, you can often just use `await main()` in a cell.\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypoYqKybJWSw",
        "outputId": "96b34610-438d-4285-b0b2-ca1973f5e819"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Installing required packages...\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n",
            "Collecting langgraph\n",
            "  Using cached langgraph-0.6.10-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting asyncio\n",
            "  Using cached asyncio-4.0.0-py3-none-any.whl.metadata (994 bytes)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (3.12.15)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.31)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.0 Requires-Python >=3.13\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement mcp-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for mcp-client\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ All packages installed successfully!\n",
            "üìö Libraries imported successfully!\n",
            "üîë Setting up API Keys and Observability...\n",
            "‚úÖ LangSmith client initialized successfully!\n",
            "‚úÖ OpenAI API key verified successfully!\n",
            "üõ†Ô∏è Initializing MCP Server...\n",
            "üîÑ Initializing A2A Communication Layer...\n",
            "üõ¢Ô∏è Initializing Observable Vector Database...\n",
            "‚úÖ Observable Vector database initialized\n",
            "üìä Generating sample data...\n",
            "üéØ Initializing Observable Retail Agent Army Orchestrator...\n",
            "‚úÖ Enhanced Agent Army Initialized: 2 observable agents\n",
            "An error occurred during execution: asyncio.run() cannot be called from a running event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2912611154.py:697: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  print(f\"An error occurred during execution: {e}\")\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k5D6hU-rJ_cD",
        "outputId": "dc145527-a6bf-4e21-88d9-0d128bf780df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Using cached chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.9)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=b1b1b6c9c68bafb656f83f07cbc741199484f33d54a391116028e8701259c2ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pybase64, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, coloredlogs, posthog, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.1.1 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.1 opentelemetry-exporter-otlp-proto-grpc-1.37.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 urllib3-2.3.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry",
                  "urllib3"
                ]
              },
              "id": "f1a88b5e96db4a3e9594a237918fc068"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "niQ_t7A1Lswt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoqvMMfELs5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "# Note: In a local environment, you would run these commands in your terminal.\n",
        "# In a Jupyter/Colab notebook, the '!' prefix executes them as shell commands.\n",
        "try:\n",
        "    # Check for essential packages\n",
        "    import langchain, chromadb, sentence_transformers, pandas, langsmith, mcp\n",
        "    print(\"‚úÖ Key packages already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required packages...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    # Install the full list of packages. 'mcp-client' and 'asyncio' are removed as they are invalid/unnecessary.\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain\", \"langchain-openai\", \"langgraph\", \"langchain-community\", \"chromadb\", \"sentence-transformers\", \"lightgbm\", \"pandas\", \"numpy\", \"matplotlib\", \"scikit-learn\", \"aiohttp\", \"langsmith\", \"guardrails-ai\"])\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# LangSmith for AI Observability\n",
        "import langsmith\n",
        "from langsmith import Client, traceable\n",
        "\n",
        "# MCP (Model Context Protocol) imports\n",
        "# This is kept for conceptual context, though no specific client functions are called.\n",
        "import mcp\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 3: Configure API Keys and Observability\n",
        "print(\"üîë Setting up API Keys and Observability...\")\n",
        "\n",
        "# It's highly recommended to use environment variables or a secrets management tool.\n",
        "# For this example, we'll use placeholders.\n",
        "# IMPORTANT: Replace these with your actual keys.\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', \"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\")\n",
        "LANGSMITH_API_KEY = os.environ.get('LANGSMITH_API_KEY', \"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\")\n",
        "LANGSMITH_PROJECT = os.environ.get('LANGSMITH_PROJECT', \"retail-agent-army\")\n",
        "\n",
        "# Set environment variables for LangChain and LangSmith\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # Enables LangSmith tracing\n",
        "\n",
        "# Initialize LangSmith client\n",
        "langsmith_client = None\n",
        "try:\n",
        "    if LANGSMITH_API_KEY != \"YOUR_LANGSMITH_API_KEY\":\n",
        "        langsmith_client = Client()\n",
        "        print(\"‚úÖ LangSmith client initialized successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è LangSmith API key not set. Skipping client initialization.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è LangSmith initialization warning: {e}\")\n",
        "\n",
        "# Test the OpenAI API key\n",
        "try:\n",
        "    if OPENAI_API_KEY != \"YOUR_OPENAI_API_KEY\":\n",
        "        test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        test_response = test_llm.invoke(\"Test\")\n",
        "        print(\"‚úÖ OpenAI API key verified successfully!\")\n",
        "    else:\n",
        "         print(\"‚ùå OpenAI API key is not set. Please replace 'YOUR_OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API key error: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: MCP Server Implementation for Retail\n",
        "class RetailMCPServer:\n",
        "    \"\"\"\n",
        "    A simulated Model Context Protocol (MCP) server that provides specialized tools\n",
        "    for retail analysis. Each tool is traceable for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"price_optimizer\": self.price_optimization_tool,\n",
        "            \"demand_forecaster\": self.demand_forecasting_tool,\n",
        "            \"inventory_optimizer\": self.inventory_optimization_tool,\n",
        "            \"customer_analyzer\": self.customer_analytics_tool,\n",
        "            \"competitor_monitor\": self.competitive_intelligence_tool,\n",
        "            \"market_analyzer\": self.market_analysis_tool\n",
        "        }\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "\n",
        "    @traceable\n",
        "    async def price_optimization_tool(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced price optimization using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Perform advanced price optimization analysis:\n",
        "\n",
        "            PRODUCT: {product_data}\n",
        "            MARKET: {market_conditions}\n",
        "\n",
        "            Calculate:\n",
        "            1. Price elasticity estimation\n",
        "            2. Optimal price point with confidence intervals\n",
        "            3. Competitive positioning strategy\n",
        "            4. Revenue maximization approach\n",
        "            5. Risk assessment\n",
        "\n",
        "            Return structured JSON analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"price_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": response.content,\n",
        "                \"recommendations\": {\n",
        "                    \"optimal_price_range\": {\"min\": 45.0, \"max\": 125.0, \"recommended\": 89.99},\n",
        "                    \"confidence_score\": 0.87,\n",
        "                    \"expected_revenue_impact\": \"+15-25%\",\n",
        "                    \"risk_level\": \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"price_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def demand_forecasting_tool(self, historical_data: List, market_signals: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Demand forecasting with market signals.\"\"\"\n",
        "        try:\n",
        "            forecast_data = {\n",
        "                \"next_30_days\": np.random.normal(100, 20, 30).tolist(),\n",
        "                \"confidence_interval\": {\"lower\": 80, \"upper\": 120},\n",
        "                \"seasonality_factor\": 1.15,\n",
        "                \"trend_direction\": \"upward\"\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"demand_forecaster\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"forecast\": forecast_data,\n",
        "                \"market_signals_used\": list(market_signals.keys()),\n",
        "                \"data_points_analyzed\": len(historical_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"demand_forecaster\"}\n",
        "\n",
        "    @traceable\n",
        "    async def inventory_optimization_tool(self, current_stock: Dict, demand_forecast: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Inventory optimization based on demand forecasting.\"\"\"\n",
        "        try:\n",
        "            optimization_recommendations = {\n",
        "                \"safety_stock_levels\": {\n",
        "                    \"high_demand\": current_stock.get('current_stock', 0) * 0.3,\n",
        "                    \"medium_demand\": current_stock.get('current_stock', 0) * 0.2,\n",
        "                    \"low_demand\": current_stock.get('current_stock', 0) * 0.1\n",
        "                },\n",
        "                \"reorder_points\": {\n",
        "                    \"urgent\": current_stock.get('reorder_point', 0) * 0.8,\n",
        "                    \"standard\": current_stock.get('reorder_point', 0),\n",
        "                    \"relaxed\": current_stock.get('reorder_point', 0) * 1.2\n",
        "                },\n",
        "                \"cost_optimization\": {\n",
        "                    \"expected_savings\": \"15-20%\",\n",
        "                    \"stockout_risk_reduction\": \"40-60%\",\n",
        "                    \"carrying_cost_optimization\": \"10-15%\"\n",
        "                }\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"inventory_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"optimization\": optimization_recommendations,\n",
        "                \"demand_forecast_incorporated\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"inventory_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def customer_analytics_tool(self, customer_data: Dict, purchase_history: List) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced customer analytics using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze customer behavior and generate insights:\n",
        "\n",
        "            CUSTOMER: {customer_data}\n",
        "            PURCHASE HISTORY: {purchase_history[:5]}\n",
        "\n",
        "            Provide:\n",
        "            1. Customer lifetime value prediction\n",
        "            2. Churn risk assessment\n",
        "            3. Personalization opportunities\n",
        "            4. Retention strategy\n",
        "            5. Cross-selling recommendations\n",
        "\n",
        "            Return structured analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"customer_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"customer_insights\": response.content,\n",
        "                \"metrics\": {\n",
        "                    \"lifetime_value_score\": self._calculate_ltv(purchase_history),\n",
        "                    \"churn_risk\": \"low\" if len(purchase_history) > 3 else \"medium\",\n",
        "                    \"engagement_level\": \"high\" if len(purchase_history) > 5 else \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"customer_analyzer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def competitive_intelligence_tool(self, product_data: Dict, market_scope: str = \"local\") -> Dict:\n",
        "        \"\"\"MCP Tool: Competitive market intelligence.\"\"\"\n",
        "        try:\n",
        "            competitor_analysis = {\n",
        "                \"main_competitors\": [\"Competitor A\", \"Competitor B\", \"Competitor C\"],\n",
        "                \"price_comparison\": {\n",
        "                    \"our_price\": product_data.get('current_price', 0),\n",
        "                    \"competitor_avg\": product_data.get('current_price', 0) * 0.9,\n",
        "                    \"market_range\": {\n",
        "                        \"min\": product_data.get('current_price', 0) * 0.7,\n",
        "                        \"max\": product_data.get('current_price', 0) * 1.3\n",
        "                    }\n",
        "                },\n",
        "                \"competitive_positioning\": \"market_leader\" if product_data.get('current_price', 0) > 100 else \"value_player\",\n",
        "                \"recommendations\": [\n",
        "                    \"Monitor competitor pricing weekly\",\n",
        "                    \"Differentiate through value-added services\",\n",
        "                    \"Consider bundle pricing strategies\"\n",
        "                ]\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"competitor_monitor\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": competitor_analysis,\n",
        "                \"market_scope\": market_scope\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"competitor_monitor\"}\n",
        "\n",
        "    @traceable\n",
        "    async def market_analysis_tool(self, product_category: str, timeframe: str = \"30d\") -> Dict:\n",
        "        \"\"\"MCP Tool: Comprehensive market analysis using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze market conditions for {product_category} over {timeframe}:\n",
        "\n",
        "            Provide:\n",
        "            1. Market trends and growth projections\n",
        "            2. Consumer behavior insights\n",
        "            3. Regulatory considerations\n",
        "            4. Technology impacts\n",
        "            5. Strategic recommendations\n",
        "\n",
        "            Focus on actionable insights for retail pricing and inventory.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"market_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"market_analysis\": response.content,\n",
        "                \"timeframe\": timeframe,\n",
        "                \"category\": product_category\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"market_analyzer\"}\n",
        "\n",
        "    def _calculate_ltv(self, purchase_history: List) -> float:\n",
        "        \"\"\"Helper function to calculate customer lifetime value.\"\"\"\n",
        "        if not purchase_history:\n",
        "            return 0.0\n",
        "        total_revenue = sum(p.get('revenue', 0) for p in purchase_history)\n",
        "        purchase_count = len(purchase_history)\n",
        "        avg_order_value = total_revenue / purchase_count if purchase_count > 0 else 0\n",
        "        return min(round(avg_order_value * purchase_count * 0.1, 2), 1000.0)\n",
        "\n",
        "print(\"üõ†Ô∏è Initializing MCP Server...\")\n",
        "mcp_server = RetailMCPServer()\n",
        "\n",
        "\n",
        "# Step 5: A2A (Agent-to-Agent) Communication Layer\n",
        "class A2ACommunicationLayer:\n",
        "    \"\"\"\n",
        "    Manages communication between agents, including direct messages, broadcasts,\n",
        "    and consensus requests, with full tracing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agent_registries = {}\n",
        "        self.conversation_history = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    @traceable\n",
        "    async def register_agent(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        \"\"\"Register an agent in the A2A network.\"\"\"\n",
        "        self.agent_registries[agent_id] = {\n",
        "            \"agent_type\": agent_type,\n",
        "            \"capabilities\": capabilities,\n",
        "            \"status\": \"active\",\n",
        "            \"last_heartbeat\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.performance_metrics[agent_id] = {\n",
        "            \"messages_sent\": 0, \"messages_received\": 0,\n",
        "            \"response_time_avg\": 0.0, \"error_rate\": 0.0\n",
        "        }\n",
        "        print(f\"‚úÖ Agent registered: {agent_id} ({agent_type})\")\n",
        "\n",
        "    @traceable\n",
        "    async def send_message(self, from_agent: str, to_agent: str, message: Dict, message_type: str = \"direct\"):\n",
        "        \"\"\"Send a message between agents with tracing.\"\"\"\n",
        "        try:\n",
        "            message_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.now().isoformat()\n",
        "\n",
        "            message_package = {\n",
        "                \"message_id\": message_id, \"from_agent\": from_agent,\n",
        "                \"to_agent\": to_agent, \"message_type\": message_type,\n",
        "                \"content\": message, \"timestamp\": timestamp\n",
        "            }\n",
        "\n",
        "            if from_agent not in self.conversation_history:\n",
        "                self.conversation_history[from_agent] = []\n",
        "            self.conversation_history[from_agent].append(message_package)\n",
        "\n",
        "            self.performance_metrics[from_agent][\"messages_sent\"] += 1\n",
        "            if to_agent in self.performance_metrics:\n",
        "                self.performance_metrics[to_agent][\"messages_received\"] += 1\n",
        "\n",
        "            print(f\"üì® A2A Message: {from_agent} ‚Üí {to_agent} ({message_type})\")\n",
        "            return message_id\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Message Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @traceable\n",
        "    async def broadcast_message(self, from_agent: str, message: Dict, agent_filter: List[str] = None):\n",
        "        \"\"\"Broadcast a message to multiple agents.\"\"\"\n",
        "        try:\n",
        "            target_agents = agent_filter if agent_filter else list(self.agent_registries.keys())\n",
        "            target_agents = [agent for agent in target_agents if agent != from_agent]\n",
        "\n",
        "            tasks = [self.send_message(from_agent, agent_id, message, \"broadcast\") for agent_id in target_agents]\n",
        "            message_ids = await asyncio.gather(*tasks)\n",
        "            return message_ids\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Broadcast Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_communication_metrics(self) -> Dict:\n",
        "        \"\"\"Get A2A communication performance metrics.\"\"\"\n",
        "        total_messages = sum(m[\"messages_sent\"] for m in self.performance_metrics.values())\n",
        "        active_agents = len([a for a, i in self.agent_registries.items() if i[\"status\"] == \"active\"])\n",
        "        return {\n",
        "            \"total_agents_registered\": len(self.agent_registries),\n",
        "            \"active_agents\": active_agents,\n",
        "            \"total_messages_exchanged\": total_messages,\n",
        "            \"agent_performance\": self.performance_metrics,\n",
        "            \"system_health\": \"optimal\" if active_agents > 0 else \"degraded\"\n",
        "        }\n",
        "\n",
        "print(\"üîÑ Initializing A2A Communication Layer...\")\n",
        "a2a_communication = A2ACommunicationLayer()\n",
        "\n",
        "\n",
        "# Step 6: Enhanced Vector Database with Observability\n",
        "class ObservableVectorDB:\n",
        "    \"\"\"\n",
        "    A wrapper for a Chroma vector database that adds observability and performance tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_store = None\n",
        "        self.query_metrics = {\"total_queries\": 0, \"successful_queries\": 0, \"average_response_time\": 0.0}\n",
        "        self.setup_sample_data()\n",
        "\n",
        "    @traceable\n",
        "    def setup_sample_data(self):\n",
        "        \"\"\"Initialize with comprehensive retail sample data.\"\"\"\n",
        "        sample_docs = [\n",
        "            Document(page_content=\"Premium pricing strategy works best for electronics with 40-60% markup\", metadata={\"category\": \"pricing\", \"product_type\": \"electronics\"}),\n",
        "            Document(page_content=\"Competitive pricing essential for smartphone accessories market\", metadata={\"category\": \"pricing\", \"product_type\": \"accessories\"}),\n",
        "            Document(page_content=\"Safety stock levels should cover 2-week demand for fast-moving goods\", metadata={\"category\": \"inventory\", \"metric\": \"safety_stock\"}),\n",
        "            Document(page_content=\"Holiday season demand increases 60-80% for electronics\", metadata={\"category\": \"trends\", \"season\": \"Q4\"}),\n",
        "            Document(page_content=\"Price sensitivity is highest in the budget segment\", metadata={\"category\": \"customer\", \"insight\": \"pricing\"})\n",
        "        ]\n",
        "        self.vector_store = Chroma.from_documents(documents=sample_docs, embedding=self.embeddings, persist_directory=\"./chroma_retail_db\")\n",
        "        print(\"‚úÖ Observable Vector database initialized\")\n",
        "\n",
        "    @traceable\n",
        "    async def enhanced_search(self, query: str, category: str = None, k: int = 3, agent_id: str = \"unknown\") -> List[Dict]:\n",
        "        \"\"\"Enhanced semantic search with observability.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.query_metrics[\"total_queries\"] += 1\n",
        "        try:\n",
        "            filter_dict = {\"category\": category} if category else {}\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.query_metrics[\"successful_queries\"] += 1\n",
        "            total_queries = self.query_metrics[\"successful_queries\"]\n",
        "            self.query_metrics[\"average_response_time\"] = ((self.query_metrics[\"average_response_time\"] * (total_queries - 1)) + response_time) / total_queries\n",
        "\n",
        "            formatted_results = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"similarity_score\": float(score)} for doc, score in results]\n",
        "            print(f\"üîç VectorDB Search: '{query}' ‚Üí {len(formatted_results)} results\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå VectorDB Search Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Get vector database performance metrics.\"\"\"\n",
        "        success_rate = self.query_metrics[\"successful_queries\"] / max(1, self.query_metrics[\"total_queries\"])\n",
        "        return {**self.query_metrics, \"success_rate\": success_rate, \"health_status\": \"optimal\"}\n",
        "\n",
        "print(\"üõ¢Ô∏è Initializing Observable Vector Database...\")\n",
        "observable_vector_db = ObservableVectorDB()\n",
        "\n",
        "\n",
        "# Step 7: Enhanced Base Agent with MCP and A2A Integration\n",
        "class EnhancedRetailAgent:\n",
        "    \"\"\"\n",
        "    A base class for retail AI agents, integrating MCP tools, A2A communication,\n",
        "    and an observable vector database.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str, agent_type: str, model: str = \"gpt-4\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = ChatOpenAI(model=model, temperature=0.1)\n",
        "        self.mcp_server = mcp_server\n",
        "        self.a2a = a2a_communication\n",
        "        self.vector_db = observable_vector_db\n",
        "        self.history = []\n",
        "        self.performance_metrics = {\n",
        "            \"tasks_completed\": 0, \"mcp_tools_used\": 0,\n",
        "            \"a2a_messages_sent\": 0, \"vector_db_queries\": 0,\n",
        "            \"avg_response_time\": 0.0, \"success_rate\": 1.0\n",
        "        }\n",
        "        # In a notebook environment, it's better to explicitly call registration\n",
        "        # after the event loop is confirmed to be running.\n",
        "        # asyncio.create_task(self._register_with_a2a())\n",
        "\n",
        "    async def _register_with_a2a(self):\n",
        "        \"\"\"Register agent with the A2A communication layer.\"\"\"\n",
        "        await self.a2a.register_agent(self.agent_id, self.agent_type, self._get_agent_capabilities())\n",
        "\n",
        "    def _get_agent_capabilities(self) -> List[str]:\n",
        "        \"\"\"Define agent capabilities for A2A registration.\"\"\"\n",
        "        caps = [\"analysis\", \"decision_support\", \"collaboration\"]\n",
        "        if \"pricing\" in self.agent_type: caps.extend([\"price_optimization\", \"competitive_analysis\"])\n",
        "        elif \"inventory\" in self.agent_type: caps.extend([\"demand_forecasting\", \"stock_optimization\"])\n",
        "        return caps\n",
        "\n",
        "    @traceable\n",
        "    async def use_mcp_tool(self, tool_name: str, tool_params: Dict) -> Dict:\n",
        "        \"\"\"Use MCP tools with built-in observability and A2A notifications.\"\"\"\n",
        "        if tool_name in self.mcp_server.tools:\n",
        "            self.performance_metrics[\"mcp_tools_used\"] += 1\n",
        "            result = await self.mcp_server.tools[tool_name](**tool_params)\n",
        "\n",
        "            # Notify other agents about tool usage\n",
        "            await self.a2a.broadcast_message(self.agent_id, {\n",
        "                \"type\": \"mcp_tool_used\", \"tool\": tool_name,\n",
        "                \"summary\": str(result)[:100] + \"...\"\n",
        "            })\n",
        "            return result\n",
        "        return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "    def get_enhanced_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive agent status with observability metrics.\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"a2a_registered\": self.agent_id in self.a2a.agent_registries,\n",
        "            \"health_status\": \"optimal\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 8: Enhanced Specialized Agents\n",
        "class EnhancedPricingAgent(EnhancedRetailAgent):\n",
        "    \"\"\"A specialized agent for pricing strategy, using all system components.\"\"\"\n",
        "    def __init__(self, agent_id: str):\n",
        "        super().__init__(agent_id, \"enhanced_pricing_strategist\", \"gpt-4\")\n",
        "\n",
        "    @traceable\n",
        "    async def analyze_pricing(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"Enhanced pricing analysis workflow.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        try:\n",
        "            # 1. Get context from Vector DB\n",
        "            self.performance_metrics[\"vector_db_queries\"] += 1\n",
        "            context = await self.vector_db.enhanced_search(\n",
        "                f\"pricing strategy for {product_data.get('category')}\",\n",
        "                category=\"pricing\", agent_id=self.agent_id\n",
        "            )\n",
        "\n",
        "            # 2. Use MCP tools for advanced analysis\n",
        "            mcp_analysis = await self.use_mcp_tool(\"price_optimizer\", {\n",
        "                \"product_data\": product_data,\n",
        "                \"market_conditions\": {**market_conditions, \"vector_context\": context}\n",
        "            })\n",
        "\n",
        "            # 3. Generate final recommendation\n",
        "            final_recommendation = await self._generate_final_recommendation(\n",
        "                product_data, mcp_analysis, context\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": final_recommendation,\n",
        "                \"mcp_analysis_used\": mcp_analysis,\n",
        "                \"context_used\": context,\n",
        "                \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    async def _generate_final_recommendation(self, product_data: Dict, mcp_analysis: Dict, context: List) -> str:\n",
        "        \"\"\"Synthesize a final recommendation using an LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Synthesize a final pricing recommendation based on the following data:\n",
        "        PRODUCT: {product_data}\n",
        "        MCP ANALYSIS: {mcp_analysis}\n",
        "        VECTOR DB CONTEXT: {context}\n",
        "\n",
        "        Provide a comprehensive pricing strategy for executive decision-making.\n",
        "        \"\"\"\n",
        "        response = await self.llm.ainvoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "\n",
        "# Step 9: Enhanced Orchestrator with Full Observability\n",
        "class ObservableRetailOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the agent army, runs workflows, and provides a centralized\n",
        "    view of system-wide observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.workflow_history = []\n",
        "        self.observability_metrics = {\n",
        "            \"total_workflows\": 0, \"successful_workflows\": 0, \"average_workflow_time\": 0.0\n",
        "        }\n",
        "\n",
        "    @traceable\n",
        "    async def setup_enhanced_agent_army(self):\n",
        "        \"\"\"Initialize the enhanced agent army and register them.\"\"\"\n",
        "        self.agents = {\n",
        "            \"pricing_alpha_enhanced\": EnhancedPricingAgent(\"pricing_alpha_enhanced\"),\n",
        "            \"pricing_beta_enhanced\": EnhancedPricingAgent(\"pricing_beta_enhanced\"),\n",
        "        }\n",
        "        # Register agents explicitly\n",
        "        for agent in self.agents.values():\n",
        "            await agent._register_with_a2a()\n",
        "\n",
        "        print(f\"‚úÖ Enhanced Agent Army Initialized and Registered: {len(self.agents)} observable agents\")\n",
        "\n",
        "    @traceable\n",
        "    async def run_observable_pricing_workflow(self, product_data: Dict) -> Dict:\n",
        "        \"\"\"Run a pricing workflow with full observability.\"\"\"\n",
        "        workflow_start = datetime.now()\n",
        "        self.observability_metrics[\"total_workflows\"] += 1\n",
        "        try:\n",
        "            print(f\"üöÄ Starting OBSERVABLE pricing workflow for {product_data.get('name')}...\")\n",
        "            market_context = await observable_vector_db.enhanced_search(f\"market analysis for {product_data.get('category')}\", k=3)\n",
        "\n",
        "            pricing_agents = [agent for aid, agent in self.agents.items() if \"pricing\" in aid]\n",
        "            tasks = [agent.analyze_pricing(product_data, {\"context\": market_context}) for agent in pricing_agents]\n",
        "            pricing_results = await asyncio.gather(*tasks)\n",
        "\n",
        "            aggregated_recommendation = await self.aggregate_enhanced_recommendations(pricing_results, product_data)\n",
        "\n",
        "            workflow_time = (datetime.now() - workflow_start).total_seconds()\n",
        "            self.observability_metrics[\"successful_workflows\"] += 1\n",
        "            total_workflows = self.observability_metrics[\"successful_workflows\"]\n",
        "            self.observability_metrics[\"average_workflow_time\"] = ((self.observability_metrics[\"average_workflow_time\"] * (total_workflows-1)) + workflow_time) / total_workflows\n",
        "\n",
        "            workflow_result = {\n",
        "                \"workflow_type\": \"observable_pricing\",\n",
        "                \"aggregated_recommendation\": aggregated_recommendation,\n",
        "                \"workflow_duration\": workflow_time\n",
        "            }\n",
        "            self.workflow_history.append(workflow_result)\n",
        "            print(f\"‚úÖ Observable pricing workflow completed in {workflow_time:.2f}s\")\n",
        "            return workflow_result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Observable workflow error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @traceable\n",
        "    async def aggregate_enhanced_recommendations(self, results: List[Dict], context_data: Dict) -> Dict:\n",
        "        \"\"\"Aggregate recommendations from multiple agents using an LLM.\"\"\"\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "        prompt = f\"\"\"\n",
        "        Aggregate these pricing recommendations from multiple AI agents:\n",
        "        CONTEXT: {context_data}\n",
        "        AGENT ANALYSES: {json.dumps(results, indent=2)}\n",
        "        Provide a single, consolidated strategy.\n",
        "        \"\"\"\n",
        "        response = await llm.ainvoke(prompt)\n",
        "        return {\"consolidated_strategy\": response.content}\n",
        "\n",
        "    def get_comprehensive_observability(self) -> Dict:\n",
        "        \"\"\"Get comprehensive observability data from all system components.\"\"\"\n",
        "        agent_statuses = {aid: agent.get_enhanced_status() for aid, agent in self.agents.items()}\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"orchestrator_metrics\": self.observability_metrics,\n",
        "            \"agent_army_status\": {\n",
        "                \"total_agents\": len(self.agents),\n",
        "                \"agent_breakdown\": {\"pricing_agents\": len([a for a in self.agents if \"pricing\" in a])}\n",
        "            },\n",
        "            \"a2a_communication_metrics\": a2a_communication.get_communication_metrics(),\n",
        "            \"vector_db_performance\": observable_vector_db.get_performance_metrics(),\n",
        "            \"workflow_history_summary\": {\n",
        "                \"total_workflows\": len(self.workflow_history),\n",
        "                \"success_rate\": self.observability_metrics[\"successful_workflows\"] / max(1, self.observability_metrics[\"total_workflows\"])\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 10: Generate Sample Data\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data for the demonstration.\"\"\"\n",
        "    return {\n",
        "        \"products\": [\n",
        "            {\"product_id\": \"PROD_001\", \"name\": \"Wireless Bluetooth Headphones\", \"category\": \"electronics\", \"cost_price\": 45.0, \"current_price\": 99.99},\n",
        "            {\"product_id\": \"PROD_002\", \"name\": \"Smartphone Protective Case\", \"category\": \"accessories\", \"cost_price\": 4.50, \"current_price\": 19.99}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"üìä Generating sample data...\")\n",
        "sample_data = generate_sample_data()\n",
        "\n",
        "# This is the main execution function.\n",
        "async def run_demo():\n",
        "    \"\"\"Initializes and runs the entire demo.\"\"\"\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
        "        print(\"\\n\\n‚ùå Cannot run demo. Please set your OpenAI API key at the top of the script.\")\n",
        "        return\n",
        "\n",
        "    # Step 11: Initialize Enhanced Orchestrator\n",
        "    print(\"üéØ Initializing Observable Retail Agent Army Orchestrator...\")\n",
        "    observable_orchestrator = ObservableRetailOrchestrator()\n",
        "    await observable_orchestrator.setup_enhanced_agent_army()\n",
        "\n",
        "\n",
        "    # Step 12: Enhanced Demo with Full Observability\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run pricing workflows\n",
        "    for product in sample_data['products']:\n",
        "        await observable_orchestrator.run_observable_pricing_workflow(product)\n",
        "\n",
        "    # Show final dashboard data\n",
        "    final_observability = observable_orchestrator.get_comprehensive_observability()\n",
        "    print(\"\\n3. üìà FINAL OBSERVABILITY DASHBOARD DATA\")\n",
        "    print(json.dumps(final_observability, indent=2, default=str))\n",
        "\n",
        "    # Step 13: Visualization and final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéä DEMO COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîç OBSERVABILITY FEATURES DEMONSTRATED:\")\n",
        "    print(\"  ‚úÖ MCP (Model Context Protocol) Integration\")\n",
        "    print(\"  ‚úÖ A2A (Agent-to-Agent) Communication Layer\")\n",
        "    print(\"  ‚úÖ LangSmith AI Observability & Tracing\")\n",
        "    print(\"  ‚úÖ Real-time Performance Monitoring\")\n",
        "\n",
        "# Execute the demo\n",
        "# In a Jupyter/Colab notebook, you would run the following line in a cell:\n",
        "# await run_demo()\n",
        "# For a script, we handle the event loop.\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # This handles the \"asyncio.run() cannot be called from a running event loop\" error\n",
        "        # by checking if a loop is already running, which is common in notebooks.\n",
        "        loop = asyncio.get_running_loop()\n",
        "        if loop.is_running():\n",
        "            print(\"Event loop is already running. Creating a task.\")\n",
        "            loop.create_task(run_demo())\n",
        "        else:\n",
        "            asyncio.run(run_demo())\n",
        "    except RuntimeError:\n",
        "        # No event loop running, so we can safely start one.\n",
        "        asyncio.run(run_demo())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during execution: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en3FmTTTLs8U",
        "outputId": "aab6afb3-c0dc-49d8-c976-364151bc6c77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Key packages already installed.\n",
            "üìö Libraries imported successfully!\n",
            "üîë Setting up API Keys and Observability...\n",
            "‚úÖ LangSmith client initialized successfully!\n",
            "‚úÖ OpenAI API key verified successfully!\n",
            "üõ†Ô∏è Initializing MCP Server...\n",
            "üîÑ Initializing A2A Communication Layer...\n",
            "üõ¢Ô∏è Initializing Observable Vector Database...\n",
            "‚úÖ Observable Vector database initialized\n",
            "üìä Generating sample data...\n",
            "Event loop is already running. Creating a task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "# Note: In a local environment, you would run these commands in your terminal.\n",
        "# In a Jupyter/Colab notebook, the '!' prefix executes them as shell commands.\n",
        "try:\n",
        "    # Check for essential packages\n",
        "    import langchain, chromadb, sentence_transformers, pandas, langsmith\n",
        "    print(\"‚úÖ Key packages already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required packages...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    # Install the full list of packages. 'mcp-client', 'langgraph', and 'asyncio' are removed as they are invalid/unnecessary.\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain\", \"langchain-openai\", \"langchain-community\", \"chromadb\", \"sentence-transformers\", \"lightgbm\", \"pandas\", \"numpy\", \"matplotlib\", \"scikit-learn\", \"aiohttp\", \"langsmith\", \"guardrails-ai\"])\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# LangSmith for AI Observability\n",
        "import langsmith\n",
        "from langsmith import Client, traceable\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 3: Configure API Keys and Observability\n",
        "print(\"üîë Setting up API Keys and Observability...\")\n",
        "\n",
        "# It's highly recommended to use environment variables or a secrets management tool.\n",
        "# For this example, we'll use placeholders.\n",
        "# IMPORTANT: Replace these with your actual keys.\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', \"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\")\n",
        "LANGSMITH_API_KEY = os.environ.get('LANGSMITH_API_KEY', \"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\")\n",
        "LANGSMITH_PROJECT = os.environ.get('LANGSMITH_PROJECT', \"retail-agent-army\")\n",
        "\n",
        "# Set environment variables for LangChain and LangSmith\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # Enables LangSmith tracing\n",
        "\n",
        "# Initialize LangSmith client\n",
        "langsmith_client = None\n",
        "try:\n",
        "    if LANGSMITH_API_KEY != \"YOUR_LANGSMITH_API_KEY\":\n",
        "        langsmith_client = Client()\n",
        "        print(\"‚úÖ LangSmith client initialized successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è LangSmith API key not set. Skipping client initialization.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è LangSmith initialization warning: {e}\")\n",
        "\n",
        "# Test the OpenAI API key\n",
        "try:\n",
        "    if OPENAI_API_KEY != \"YOUR_OPENAI_API_KEY\":\n",
        "        test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        test_response = test_llm.invoke(\"Test\")\n",
        "        print(\"‚úÖ OpenAI API key verified successfully!\")\n",
        "    else:\n",
        "         print(\"‚ùå OpenAI API key is not set. Please replace 'YOUR_OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API key error: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: MCP Server Implementation for Retail\n",
        "class RetailMCPServer:\n",
        "    \"\"\"\n",
        "    A simulated Model Context Protocol (MCP) server that provides specialized tools\n",
        "    for retail analysis. Each tool is traceable for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"price_optimizer\": self.price_optimization_tool,\n",
        "            \"demand_forecaster\": self.demand_forecasting_tool,\n",
        "            \"inventory_optimizer\": self.inventory_optimization_tool,\n",
        "            \"customer_analyzer\": self.customer_analytics_tool,\n",
        "            \"competitor_monitor\": self.competitive_intelligence_tool,\n",
        "            \"market_analyzer\": self.market_analysis_tool\n",
        "        }\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "\n",
        "    @traceable\n",
        "    async def price_optimization_tool(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced price optimization using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Perform advanced price optimization analysis:\n",
        "\n",
        "            PRODUCT: {product_data}\n",
        "            MARKET: {market_conditions}\n",
        "\n",
        "            Calculate:\n",
        "            1. Price elasticity estimation\n",
        "            2. Optimal price point with confidence intervals\n",
        "            3. Competitive positioning strategy\n",
        "            4. Revenue maximization approach\n",
        "            5. Risk assessment\n",
        "\n",
        "            Return structured JSON analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"price_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": response.content,\n",
        "                \"recommendations\": {\n",
        "                    \"optimal_price_range\": {\"min\": 45.0, \"max\": 125.0, \"recommended\": 89.99},\n",
        "                    \"confidence_score\": 0.87,\n",
        "                    \"expected_revenue_impact\": \"+15-25%\",\n",
        "                    \"risk_level\": \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"price_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def demand_forecasting_tool(self, historical_data: List, market_signals: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Demand forecasting with market signals.\"\"\"\n",
        "        try:\n",
        "            forecast_data = {\n",
        "                \"next_30_days\": np.random.normal(100, 20, 30).tolist(),\n",
        "                \"confidence_interval\": {\"lower\": 80, \"upper\": 120},\n",
        "                \"seasonality_factor\": 1.15,\n",
        "                \"trend_direction\": \"upward\"\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"demand_forecaster\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"forecast\": forecast_data,\n",
        "                \"market_signals_used\": list(market_signals.keys()),\n",
        "                \"data_points_analyzed\": len(historical_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"demand_forecaster\"}\n",
        "\n",
        "    @traceable\n",
        "    async def inventory_optimization_tool(self, current_stock: Dict, demand_forecast: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Inventory optimization based on demand forecasting.\"\"\"\n",
        "        try:\n",
        "            optimization_recommendations = {\n",
        "                \"safety_stock_levels\": {\n",
        "                    \"high_demand\": current_stock.get('current_stock', 0) * 0.3,\n",
        "                    \"medium_demand\": current_stock.get('current_stock', 0) * 0.2,\n",
        "                    \"low_demand\": current_stock.get('current_stock', 0) * 0.1\n",
        "                },\n",
        "                \"reorder_points\": {\n",
        "                    \"urgent\": current_stock.get('reorder_point', 0) * 0.8,\n",
        "                    \"standard\": current_stock.get('reorder_point', 0),\n",
        "                    \"relaxed\": current_stock.get('reorder_point', 0) * 1.2\n",
        "                },\n",
        "                \"cost_optimization\": {\n",
        "                    \"expected_savings\": \"15-20%\",\n",
        "                    \"stockout_risk_reduction\": \"40-60%\",\n",
        "                    \"carrying_cost_optimization\": \"10-15%\"\n",
        "                }\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"inventory_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"optimization\": optimization_recommendations,\n",
        "                \"demand_forecast_incorporated\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"inventory_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def customer_analytics_tool(self, customer_data: Dict, purchase_history: List) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced customer analytics using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze customer behavior and generate insights:\n",
        "\n",
        "            CUSTOMER: {customer_data}\n",
        "            PURCHASE HISTORY: {purchase_history[:5]}\n",
        "\n",
        "            Provide:\n",
        "            1. Customer lifetime value prediction\n",
        "            2. Churn risk assessment\n",
        "            3. Personalization opportunities\n",
        "            4. Retention strategy\n",
        "            5. Cross-selling recommendations\n",
        "\n",
        "            Return structured analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"customer_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"customer_insights\": response.content,\n",
        "                \"metrics\": {\n",
        "                    \"lifetime_value_score\": self._calculate_ltv(purchase_history),\n",
        "                    \"churn_risk\": \"low\" if len(purchase_history) > 3 else \"medium\",\n",
        "                    \"engagement_level\": \"high\" if len(purchase_history) > 5 else \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"customer_analyzer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def competitive_intelligence_tool(self, product_data: Dict, market_scope: str = \"local\") -> Dict:\n",
        "        \"\"\"MCP Tool: Competitive market intelligence.\"\"\"\n",
        "        try:\n",
        "            competitor_analysis = {\n",
        "                \"main_competitors\": [\"Competitor A\", \"Competitor B\", \"Competitor C\"],\n",
        "                \"price_comparison\": {\n",
        "                    \"our_price\": product_data.get('current_price', 0),\n",
        "                    \"competitor_avg\": product_data.get('current_price', 0) * 0.9,\n",
        "                    \"market_range\": {\n",
        "                        \"min\": product_data.get('current_price', 0) * 0.7,\n",
        "                        \"max\": product_data.get('current_price', 0) * 1.3\n",
        "                    }\n",
        "                },\n",
        "                \"competitive_positioning\": \"market_leader\" if product_data.get('current_price', 0) > 100 else \"value_player\",\n",
        "                \"recommendations\": [\n",
        "                    \"Monitor competitor pricing weekly\",\n",
        "                    \"Differentiate through value-added services\",\n",
        "                    \"Consider bundle pricing strategies\"\n",
        "                ]\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"competitor_monitor\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": competitor_analysis,\n",
        "                \"market_scope\": market_scope\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"competitor_monitor\"}\n",
        "\n",
        "    @traceable\n",
        "    async def market_analysis_tool(self, product_category: str, timeframe: str = \"30d\") -> Dict:\n",
        "        \"\"\"MCP Tool: Comprehensive market analysis using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze market conditions for {product_category} over {timeframe}:\n",
        "\n",
        "            Provide:\n",
        "            1. Market trends and growth projections\n",
        "            2. Consumer behavior insights\n",
        "            3. Regulatory considerations\n",
        "            4. Technology impacts\n",
        "            5. Strategic recommendations\n",
        "\n",
        "            Focus on actionable insights for retail pricing and inventory.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"market_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"market_analysis\": response.content,\n",
        "                \"timeframe\": timeframe,\n",
        "                \"category\": product_category\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"market_analyzer\"}\n",
        "\n",
        "    def _calculate_ltv(self, purchase_history: List) -> float:\n",
        "        \"\"\"Helper function to calculate customer lifetime value.\"\"\"\n",
        "        if not purchase_history:\n",
        "            return 0.0\n",
        "        total_revenue = sum(p.get('revenue', 0) for p in purchase_history)\n",
        "        purchase_count = len(purchase_history)\n",
        "        avg_order_value = total_revenue / purchase_count if purchase_count > 0 else 0\n",
        "        return min(round(avg_order_value * purchase_count * 0.1, 2), 1000.0)\n",
        "\n",
        "print(\"üõ†Ô∏è Initializing MCP Server...\")\n",
        "mcp_server = RetailMCPServer()\n",
        "\n",
        "\n",
        "# Step 5: A2A (Agent-to-Agent) Communication Layer\n",
        "class A2ACommunicationLayer:\n",
        "    \"\"\"\n",
        "    Manages communication between agents, including direct messages, broadcasts,\n",
        "    and consensus requests, with full tracing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agent_registries = {}\n",
        "        self.conversation_history = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    @traceable\n",
        "    async def register_agent(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        \"\"\"Register an agent in the A2A network.\"\"\"\n",
        "        self.agent_registries[agent_id] = {\n",
        "            \"agent_type\": agent_type,\n",
        "            \"capabilities\": capabilities,\n",
        "            \"status\": \"active\",\n",
        "            \"last_heartbeat\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.performance_metrics[agent_id] = {\n",
        "            \"messages_sent\": 0, \"messages_received\": 0,\n",
        "            \"response_time_avg\": 0.0, \"error_rate\": 0.0\n",
        "        }\n",
        "        print(f\"‚úÖ Agent registered: {agent_id} ({agent_type})\")\n",
        "\n",
        "    @traceable\n",
        "    async def send_message(self, from_agent: str, to_agent: str, message: Dict, message_type: str = \"direct\"):\n",
        "        \"\"\"Send a message between agents with tracing.\"\"\"\n",
        "        try:\n",
        "            message_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.now().isoformat()\n",
        "\n",
        "            message_package = {\n",
        "                \"message_id\": message_id, \"from_agent\": from_agent,\n",
        "                \"to_agent\": to_agent, \"message_type\": message_type,\n",
        "                \"content\": message, \"timestamp\": timestamp\n",
        "            }\n",
        "\n",
        "            if from_agent not in self.conversation_history:\n",
        "                self.conversation_history[from_agent] = []\n",
        "            self.conversation_history[from_agent].append(message_package)\n",
        "\n",
        "            self.performance_metrics[from_agent][\"messages_sent\"] += 1\n",
        "            if to_agent in self.performance_metrics:\n",
        "                self.performance_metrics[to_agent][\"messages_received\"] += 1\n",
        "\n",
        "            print(f\"üì® A2A Message: {from_agent} ‚Üí {to_agent} ({message_type})\")\n",
        "            return message_id\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Message Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @traceable\n",
        "    async def broadcast_message(self, from_agent: str, message: Dict, agent_filter: List[str] = None):\n",
        "        \"\"\"Broadcast a message to multiple agents.\"\"\"\n",
        "        try:\n",
        "            target_agents = agent_filter if agent_filter else list(self.agent_registries.keys())\n",
        "            target_agents = [agent for agent in target_agents if agent != from_agent]\n",
        "\n",
        "            tasks = [self.send_message(from_agent, agent_id, message, \"broadcast\") for agent_id in target_agents]\n",
        "            message_ids = await asyncio.gather(*tasks)\n",
        "            return message_ids\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Broadcast Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_communication_metrics(self) -> Dict:\n",
        "        \"\"\"Get A2A communication performance metrics.\"\"\"\n",
        "        total_messages = sum(m[\"messages_sent\"] for m in self.performance_metrics.values())\n",
        "        active_agents = len([a for a, i in self.agent_registries.items() if i[\"status\"] == \"active\"])\n",
        "        return {\n",
        "            \"total_agents_registered\": len(self.agent_registries),\n",
        "            \"active_agents\": active_agents,\n",
        "            \"total_messages_exchanged\": total_messages,\n",
        "            \"agent_performance\": self.performance_metrics,\n",
        "            \"system_health\": \"optimal\" if active_agents > 0 else \"degraded\"\n",
        "        }\n",
        "\n",
        "print(\"üîÑ Initializing A2A Communication Layer...\")\n",
        "a2a_communication = A2ACommunicationLayer()\n",
        "\n",
        "\n",
        "# Step 6: Enhanced Vector Database with Observability\n",
        "class ObservableVectorDB:\n",
        "    \"\"\"\n",
        "    A wrapper for a Chroma vector database that adds observability and performance tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_store = None\n",
        "        self.query_metrics = {\"total_queries\": 0, \"successful_queries\": 0, \"average_response_time\": 0.0}\n",
        "        self.setup_sample_data()\n",
        "\n",
        "    @traceable\n",
        "    def setup_sample_data(self):\n",
        "        \"\"\"Initialize with comprehensive retail sample data.\"\"\"\n",
        "        sample_docs = [\n",
        "            Document(page_content=\"Premium pricing strategy works best for electronics with 40-60% markup\", metadata={\"category\": \"pricing\", \"product_type\": \"electronics\"}),\n",
        "            Document(page_content=\"Competitive pricing essential for smartphone accessories market\", metadata={\"category\": \"pricing\", \"product_type\": \"accessories\"}),\n",
        "            Document(page_content=\"Safety stock levels should cover 2-week demand for fast-moving goods\", metadata={\"category\": \"inventory\", \"metric\": \"safety_stock\"}),\n",
        "            Document(page_content=\"Holiday season demand increases 60-80% for electronics\", metadata={\"category\": \"trends\", \"season\": \"Q4\"}),\n",
        "            Document(page_content=\"Price sensitivity is highest in the budget segment\", metadata={\"category\": \"customer\", \"insight\": \"pricing\"})\n",
        "        ]\n",
        "        self.vector_store = Chroma.from_documents(documents=sample_docs, embedding=self.embeddings, persist_directory=\"./chroma_retail_db\")\n",
        "        print(\"‚úÖ Observable Vector database initialized\")\n",
        "\n",
        "    @traceable\n",
        "    async def enhanced_search(self, query: str, category: str = None, k: int = 3, agent_id: str = \"unknown\") -> List[Dict]:\n",
        "        \"\"\"Enhanced semantic search with observability.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.query_metrics[\"total_queries\"] += 1\n",
        "        try:\n",
        "            filter_dict = {\"category\": category} if category else {}\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.query_metrics[\"successful_queries\"] += 1\n",
        "            total_queries = self.query_metrics[\"successful_queries\"]\n",
        "            self.query_metrics[\"average_response_time\"] = ((self.query_metrics[\"average_response_time\"] * (total_queries - 1)) + response_time) / total_queries\n",
        "\n",
        "            formatted_results = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"similarity_score\": float(score)} for doc, score in results]\n",
        "            print(f\"üîç VectorDB Search: '{query}' ‚Üí {len(formatted_results)} results\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå VectorDB Search Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Get vector database performance metrics.\"\"\"\n",
        "        success_rate = self.query_metrics[\"successful_queries\"] / max(1, self.query_metrics[\"total_queries\"])\n",
        "        return {**self.query_metrics, \"success_rate\": success_rate, \"health_status\": \"optimal\"}\n",
        "\n",
        "print(\"üõ¢Ô∏è Initializing Observable Vector Database...\")\n",
        "observable_vector_db = ObservableVectorDB()\n",
        "\n",
        "\n",
        "# Step 7: Enhanced Base Agent with MCP and A2A Integration\n",
        "class EnhancedRetailAgent:\n",
        "    \"\"\"\n",
        "    A base class for retail AI agents, integrating MCP tools, A2A communication,\n",
        "    and an observable vector database.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str, agent_type: str, model: str = \"gpt-4\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = ChatOpenAI(model=model, temperature=0.1)\n",
        "        self.mcp_server = mcp_server\n",
        "        self.a2a = a2a_communication\n",
        "        self.vector_db = observable_vector_db\n",
        "        self.history = []\n",
        "        self.performance_metrics = {\n",
        "            \"tasks_completed\": 0, \"mcp_tools_used\": 0,\n",
        "            \"a2a_messages_sent\": 0, \"vector_db_queries\": 0,\n",
        "            \"avg_response_time\": 0.0, \"success_rate\": 1.0\n",
        "        }\n",
        "\n",
        "    async def register(self):\n",
        "        \"\"\"Register agent with the A2A communication layer.\"\"\"\n",
        "        await self.a2a.register_agent(self.agent_id, self.agent_type, self._get_agent_capabilities())\n",
        "\n",
        "    def _get_agent_capabilities(self) -> List[str]:\n",
        "        \"\"\"Define agent capabilities for A2A registration.\"\"\"\n",
        "        caps = [\"analysis\", \"decision_support\", \"collaboration\"]\n",
        "        if \"pricing\" in self.agent_type: caps.extend([\"price_optimization\", \"competitive_analysis\"])\n",
        "        elif \"inventory\" in self.agent_type: caps.extend([\"demand_forecasting\", \"stock_optimization\"])\n",
        "        return caps\n",
        "\n",
        "    @traceable\n",
        "    async def use_mcp_tool(self, tool_name: str, tool_params: Dict) -> Dict:\n",
        "        \"\"\"Use MCP tools with built-in observability and A2A notifications.\"\"\"\n",
        "        if tool_name in self.mcp_server.tools:\n",
        "            self.performance_metrics[\"mcp_tools_used\"] += 1\n",
        "            result = await self.mcp_server.tools[tool_name](**tool_params)\n",
        "\n",
        "            # Notify other agents about tool usage\n",
        "            await self.a2a.broadcast_message(self.agent_id, {\n",
        "                \"type\": \"mcp_tool_used\", \"tool\": tool_name,\n",
        "                \"summary\": str(result)[:100] + \"...\"\n",
        "            })\n",
        "            return result\n",
        "        return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "    def get_enhanced_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive agent status with observability metrics.\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"a2a_registered\": self.agent_id in self.a2a.agent_registries,\n",
        "            \"health_status\": \"optimal\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 8: Enhanced Specialized Agents\n",
        "class EnhancedPricingAgent(EnhancedRetailAgent):\n",
        "    \"\"\"A specialized agent for pricing strategy, using all system components.\"\"\"\n",
        "    def __init__(self, agent_id: str):\n",
        "        super().__init__(agent_id, \"enhanced_pricing_strategist\", \"gpt-4\")\n",
        "\n",
        "    @traceable\n",
        "    async def analyze_pricing(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"Enhanced pricing analysis workflow.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        try:\n",
        "            # 1. Get context from Vector DB\n",
        "            self.performance_metrics[\"vector_db_queries\"] += 1\n",
        "            context = await self.vector_db.enhanced_search(\n",
        "                f\"pricing strategy for {product_data.get('category')}\",\n",
        "                category=\"pricing\", agent_id=self.agent_id\n",
        "            )\n",
        "\n",
        "            # 2. Use MCP tools for advanced analysis\n",
        "            mcp_analysis = await self.use_mcp_tool(\"price_optimizer\", {\n",
        "                \"product_data\": product_data,\n",
        "                \"market_conditions\": {**market_conditions, \"vector_context\": context}\n",
        "            })\n",
        "\n",
        "            # 3. Generate final recommendation\n",
        "            final_recommendation = await self._generate_final_recommendation(\n",
        "                product_data, mcp_analysis, context\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": final_recommendation,\n",
        "                \"mcp_analysis_used\": mcp_analysis,\n",
        "                \"context_used\": context,\n",
        "                \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    async def _generate_final_recommendation(self, product_data: Dict, mcp_analysis: Dict, context: List) -> str:\n",
        "        \"\"\"Synthesize a final recommendation using an LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Synthesize a final pricing recommendation based on the following data:\n",
        "        PRODUCT: {product_data}\n",
        "        MCP ANALYSIS: {mcp_analysis}\n",
        "        VECTOR DB CONTEXT: {context}\n",
        "\n",
        "        Provide a comprehensive pricing strategy for executive decision-making.\n",
        "        \"\"\"\n",
        "        response = await self.llm.ainvoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "\n",
        "# Step 9: Enhanced Orchestrator with Full Observability\n",
        "class ObservableRetailOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the agent army, runs workflows, and provides a centralized\n",
        "    view of system-wide observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.workflow_history = []\n",
        "        self.observability_metrics = {\n",
        "            \"total_workflows\": 0, \"successful_workflows\": 0, \"average_workflow_time\": 0.0\n",
        "        }\n",
        "\n",
        "    @traceable\n",
        "    async def setup_enhanced_agent_army(self):\n",
        "        \"\"\"Initialize the enhanced agent army and register them.\"\"\"\n",
        "        self.agents = {\n",
        "            \"pricing_alpha_enhanced\": EnhancedPricingAgent(\"pricing_alpha_enhanced\"),\n",
        "            \"pricing_beta_enhanced\": EnhancedPricingAgent(\"pricing_beta_enhanced\"),\n",
        "        }\n",
        "        # Register agents explicitly\n",
        "        for agent in self.agents.values():\n",
        "            await agent.register()\n",
        "\n",
        "        print(f\"‚úÖ Enhanced Agent Army Initialized and Registered: {len(self.agents)} observable agents\")\n",
        "\n",
        "    @traceable\n",
        "    async def run_observable_pricing_workflow(self, product_data: Dict) -> Dict:\n",
        "        \"\"\"Run a pricing workflow with full observability.\"\"\"\n",
        "        workflow_start = datetime.now()\n",
        "        self.observability_metrics[\"total_workflows\"] += 1\n",
        "        try:\n",
        "            print(f\"üöÄ Starting OBSERVABLE pricing workflow for {product_data.get('name')}...\")\n",
        "            market_context = await observable_vector_db.enhanced_search(f\"market analysis for {product_data.get('category')}\", k=3)\n",
        "\n",
        "            pricing_agents = [agent for aid, agent in self.agents.items() if \"pricing\" in aid]\n",
        "            tasks = [agent.analyze_pricing(product_data, {\"context\": market_context}) for agent in pricing_agents]\n",
        "            pricing_results = await asyncio.gather(*tasks)\n",
        "\n",
        "            aggregated_recommendation = await self.aggregate_enhanced_recommendations(pricing_results, product_data)\n",
        "\n",
        "            workflow_time = (datetime.now() - workflow_start).total_seconds()\n",
        "            self.observability_metrics[\"successful_workflows\"] += 1\n",
        "            total_workflows = self.observability_metrics[\"successful_workflows\"]\n",
        "            self.observability_metrics[\"average_workflow_time\"] = ((self.observability_metrics[\"average_workflow_time\"] * (total_workflows-1)) + workflow_time) / total_workflows\n",
        "\n",
        "            workflow_result = {\n",
        "                \"workflow_type\": \"observable_pricing\",\n",
        "                \"aggregated_recommendation\": aggregated_recommendation,\n",
        "                \"workflow_duration\": workflow_time\n",
        "            }\n",
        "            self.workflow_history.append(workflow_result)\n",
        "            print(f\"‚úÖ Observable pricing workflow completed in {workflow_time:.2f}s\")\n",
        "            return workflow_result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Observable workflow error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @traceable\n",
        "    async def aggregate_enhanced_recommendations(self, results: List[Dict], context_data: Dict) -> Dict:\n",
        "        \"\"\"Aggregate recommendations from multiple agents using an LLM.\"\"\"\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "        prompt = f\"\"\"\n",
        "        Aggregate these pricing recommendations from multiple AI agents:\n",
        "        CONTEXT: {context_data}\n",
        "        AGENT ANALYSES: {json.dumps(results, indent=2)}\n",
        "        Provide a single, consolidated strategy.\n",
        "        \"\"\"\n",
        "        response = await llm.ainvoke(prompt)\n",
        "        return {\"consolidated_strategy\": response.content}\n",
        "\n",
        "    def get_comprehensive_observability(self) -> Dict:\n",
        "        \"\"\"Get comprehensive observability data from all system components.\"\"\"\n",
        "        agent_statuses = {aid: agent.get_enhanced_status() for aid, agent in self.agents.items()}\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"orchestrator_metrics\": self.observability_metrics,\n",
        "            \"agent_army_status\": {\n",
        "                \"total_agents\": len(self.agents),\n",
        "                \"agent_breakdown\": {\"pricing_agents\": len([a for a in self.agents if \"pricing\" in a])}\n",
        "            },\n",
        "            \"a2a_communication_metrics\": a2a_communication.get_communication_metrics(),\n",
        "            \"vector_db_performance\": observable_vector_db.get_performance_metrics(),\n",
        "            \"workflow_history_summary\": {\n",
        "                \"total_workflows\": len(self.workflow_history),\n",
        "                \"success_rate\": self.observability_metrics[\"successful_workflows\"] / max(1, self.observability_metrics[\"total_workflows\"])\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 10: Generate Sample Data\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data for the demonstration.\"\"\"\n",
        "    return {\n",
        "        \"products\": [\n",
        "            {\"product_id\": \"PROD_001\", \"name\": \"Wireless Bluetooth Headphones\", \"category\": \"electronics\", \"cost_price\": 45.0, \"current_price\": 99.99},\n",
        "            {\"product_id\": \"PROD_002\", \"name\": \"Smartphone Protective Case\", \"category\": \"accessories\", \"cost_price\": 4.50, \"current_price\": 19.99}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"üìä Generating sample data...\")\n",
        "sample_data = generate_sample_data()\n",
        "\n",
        "# Step 11: Main Execution Function\n",
        "async def main():\n",
        "    \"\"\"Initializes and runs the entire demo.\"\"\"\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
        "        print(\"\\n\\n‚ùå Cannot run demo. Please set your OpenAI API key at the top of the script.\")\n",
        "        return\n",
        "\n",
        "    # Initialize Enhanced Orchestrator\n",
        "    print(\"üéØ Initializing Observable Retail Agent Army Orchestrator...\")\n",
        "    observable_orchestrator = ObservableRetailOrchestrator()\n",
        "    await observable_orchestrator.setup_enhanced_agent_army()\n",
        "\n",
        "\n",
        "    # Run Enhanced Demo with Full Observability\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run pricing workflows\n",
        "    for product in sample_data['products']:\n",
        "        await observable_orchestrator.run_observable_pricing_workflow(product)\n",
        "\n",
        "    # Show final dashboard data\n",
        "    final_observability = observable_orchestrator.get_comprehensive_observability()\n",
        "    print(\"\\n3. üìà FINAL OBSERVABILITY DASHBOARD DATA\")\n",
        "    print(json.dumps(final_observability, indent=2, default=str))\n",
        "\n",
        "    # Visualization and final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéä DEMO COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîç OBSERVABILITY FEATURES DEMONSTRATED:\")\n",
        "    print(\"  ‚úÖ MCP (Model Context Protocol) Integration\")\n",
        "    print(\"  ‚úÖ A2A (Agent-to-Agent) Communication Layer\")\n",
        "    print(\"  ‚úÖ LangSmith AI Observability & Tracing\")\n",
        "    print(\"  ‚úÖ Real-time Performance Monitoring\")\n",
        "\n",
        "# Step 12: Run the Demo\n",
        "# In a Jupyter/Colab notebook, this final block should be in its own cell.\n",
        "# The `asyncio.run()` function starts the event loop and runs the `main` coroutine.\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the demo run: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_IN6YyEMdBP",
        "outputId": "8de24602-2bb0-4c51-cc08-1c514c838a5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Key packages already installed.\n",
            "üìö Libraries imported successfully!\n",
            "üîë Setting up API Keys and Observability...\n",
            "‚úÖ LangSmith client initialized successfully!\n",
            "‚úÖ OpenAI API key verified successfully!\n",
            "üõ†Ô∏è Initializing MCP Server...\n",
            "üîÑ Initializing A2A Communication Layer...\n",
            "üõ¢Ô∏è Initializing Observable Vector Database...\n",
            "‚úÖ Observable Vector database initialized\n",
            "üìä Generating sample data...\n",
            "An error occurred during the demo run: asyncio.run() cannot be called from a running event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-572815934.py:687: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  print(f\"An error occurred during the demo run: {e}\")\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RWNSVuRNJJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "# Note: In a local environment, you would run these commands in your terminal.\n",
        "# In a Jupyter/Colab notebook, the '!' prefix executes them as shell commands.\n",
        "try:\n",
        "    # Check for essential packages\n",
        "    import langchain, chromadb, sentence_transformers, pandas, langsmith, mcp, nest_asyncio\n",
        "    print(\"‚úÖ Key packages already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required packages...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    # Install the full list of packages, including nest_asyncio to handle event loop issues.\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain\", \"langchain-openai\", \"langchain-community\", \"chromadb\", \"sentence-transformers\", \"lightgbm\", \"pandas\", \"numpy\", \"matplotlib\", \"scikit-learn\", \"aiohttp\", \"langsmith\", \"guardrails-ai\", \"nest_asyncio\"])\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply the patch to allow nested event loops, which is common in notebooks.\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# LangSmith for AI Observability\n",
        "import langsmith\n",
        "from langsmith import Client, traceable\n",
        "\n",
        "# MCP (Model Context Protocol) conceptual import\n",
        "import mcp\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 3: Configure API Keys and Observability\n",
        "print(\"üîë Setting up API Keys and Observability...\")\n",
        "\n",
        "# It's highly recommended to use environment variables or a secrets management tool.\n",
        "# For this example, we'll use placeholders.\n",
        "# IMPORTANT: Replace these with your actual keys.\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', \"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\")\n",
        "LANGSMITH_API_KEY = os.environ.get('LANGSMITH_API_KEY', \"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\")\n",
        "LANGSMITH_PROJECT = os.environ.get('LANGSMITH_PROJECT', \"retail-agents\")\n",
        "\n",
        "# Set environment variables for LangChain and LangSmith\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # Enables LangSmith tracing\n",
        "\n",
        "# Initialize LangSmith client\n",
        "langsmith_client = None\n",
        "try:\n",
        "    if LANGSMITH_API_KEY != \"YOUR_LANGSMITH_API_KEY\":\n",
        "        langsmith_client = Client()\n",
        "        print(\"‚úÖ LangSmith client initialized successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è LangSmith API key not set. Skipping client initialization.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è LangSmith initialization warning: {e}\")\n",
        "\n",
        "# Test the OpenAI API key\n",
        "try:\n",
        "    if OPENAI_API_KEY != \"YOUR_OPENAI_API_KEY\":\n",
        "        test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        test_response = test_llm.invoke(\"Test\")\n",
        "        print(\"‚úÖ OpenAI API key verified successfully!\")\n",
        "    else:\n",
        "         print(\"‚ùå OpenAI API key is not set. Please replace 'YOUR_OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API key error: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: MCP Server Implementation for Retail\n",
        "class RetailMCPServer:\n",
        "    \"\"\"\n",
        "    A simulated Model Context Protocol (MCP) server that provides specialized tools\n",
        "    for retail analysis. Each tool is traceable for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"price_optimizer\": self.price_optimization_tool,\n",
        "            \"demand_forecaster\": self.demand_forecasting_tool,\n",
        "            \"inventory_optimizer\": self.inventory_optimization_tool,\n",
        "            \"customer_analyzer\": self.customer_analytics_tool,\n",
        "            \"competitor_monitor\": self.competitive_intelligence_tool,\n",
        "            \"market_analyzer\": self.market_analysis_tool\n",
        "        }\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "\n",
        "    @traceable\n",
        "    async def price_optimization_tool(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced price optimization using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Perform advanced price optimization analysis:\n",
        "\n",
        "            PRODUCT: {product_data}\n",
        "            MARKET: {market_conditions}\n",
        "\n",
        "            Calculate:\n",
        "            1. Price elasticity estimation\n",
        "            2. Optimal price point with confidence intervals\n",
        "            3. Competitive positioning strategy\n",
        "            4. Revenue maximization approach\n",
        "            5. Risk assessment\n",
        "\n",
        "            Return structured JSON analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"price_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": response.content,\n",
        "                \"recommendations\": {\n",
        "                    \"optimal_price_range\": {\"min\": 45.0, \"max\": 125.0, \"recommended\": 89.99},\n",
        "                    \"confidence_score\": 0.87,\n",
        "                    \"expected_revenue_impact\": \"+15-25%\",\n",
        "                    \"risk_level\": \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"price_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def demand_forecasting_tool(self, historical_data: List, market_signals: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Demand forecasting with market signals.\"\"\"\n",
        "        try:\n",
        "            forecast_data = {\n",
        "                \"next_30_days\": np.random.normal(100, 20, 30).tolist(),\n",
        "                \"confidence_interval\": {\"lower\": 80, \"upper\": 120},\n",
        "                \"seasonality_factor\": 1.15,\n",
        "                \"trend_direction\": \"upward\"\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"demand_forecaster\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"forecast\": forecast_data,\n",
        "                \"market_signals_used\": list(market_signals.keys()),\n",
        "                \"data_points_analyzed\": len(historical_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"demand_forecaster\"}\n",
        "\n",
        "    @traceable\n",
        "    async def inventory_optimization_tool(self, current_stock: Dict, demand_forecast: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Inventory optimization based on demand forecasting.\"\"\"\n",
        "        try:\n",
        "            optimization_recommendations = {\n",
        "                \"safety_stock_levels\": {\n",
        "                    \"high_demand\": current_stock.get('current_stock', 0) * 0.3,\n",
        "                    \"medium_demand\": current_stock.get('current_stock', 0) * 0.2,\n",
        "                    \"low_demand\": current_stock.get('current_stock', 0) * 0.1\n",
        "                },\n",
        "                \"reorder_points\": {\n",
        "                    \"urgent\": current_stock.get('reorder_point', 0) * 0.8,\n",
        "                    \"standard\": current_stock.get('reorder_point', 0),\n",
        "                    \"relaxed\": current_stock.get('reorder_point', 0) * 1.2\n",
        "                },\n",
        "                \"cost_optimization\": {\n",
        "                    \"expected_savings\": \"15-20%\",\n",
        "                    \"stockout_risk_reduction\": \"40-60%\",\n",
        "                    \"carrying_cost_optimization\": \"10-15%\"\n",
        "                }\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"inventory_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"optimization\": optimization_recommendations,\n",
        "                \"demand_forecast_incorporated\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"inventory_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def customer_analytics_tool(self, customer_data: Dict, purchase_history: List) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced customer analytics using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze customer behavior and generate insights:\n",
        "\n",
        "            CUSTOMER: {customer_data}\n",
        "            PURCHASE HISTORY: {purchase_history[:5]}\n",
        "\n",
        "            Provide:\n",
        "            1. Customer lifetime value prediction\n",
        "            2. Churn risk assessment\n",
        "            3. Personalization opportunities\n",
        "            4. Retention strategy\n",
        "            5. Cross-selling recommendations\n",
        "\n",
        "            Return structured analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"customer_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"customer_insights\": response.content,\n",
        "                \"metrics\": {\n",
        "                    \"lifetime_value_score\": self._calculate_ltv(purchase_history),\n",
        "                    \"churn_risk\": \"low\" if len(purchase_history) > 3 else \"medium\",\n",
        "                    \"engagement_level\": \"high\" if len(purchase_history) > 5 else \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"customer_analyzer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def competitive_intelligence_tool(self, product_data: Dict, market_scope: str = \"local\") -> Dict:\n",
        "        \"\"\"MCP Tool: Competitive market intelligence.\"\"\"\n",
        "        try:\n",
        "            competitor_analysis = {\n",
        "                \"main_competitors\": [\"Competitor A\", \"Competitor B\", \"Competitor C\"],\n",
        "                \"price_comparison\": {\n",
        "                    \"our_price\": product_data.get('current_price', 0),\n",
        "                    \"competitor_avg\": product_data.get('current_price', 0) * 0.9,\n",
        "                    \"market_range\": {\n",
        "                        \"min\": product_data.get('current_price', 0) * 0.7,\n",
        "                        \"max\": product_data.get('current_price', 0) * 1.3\n",
        "                    }\n",
        "                },\n",
        "                \"competitive_positioning\": \"market_leader\" if product_data.get('current_price', 0) > 100 else \"value_player\",\n",
        "                \"recommendations\": [\n",
        "                    \"Monitor competitor pricing weekly\",\n",
        "                    \"Differentiate through value-added services\",\n",
        "                    \"Consider bundle pricing strategies\"\n",
        "                ]\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"competitor_monitor\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": competitor_analysis,\n",
        "                \"market_scope\": market_scope\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"competitor_monitor\"}\n",
        "\n",
        "    @traceable\n",
        "    async def market_analysis_tool(self, product_category: str, timeframe: str = \"30d\") -> Dict:\n",
        "        \"\"\"MCP Tool: Comprehensive market analysis using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze market conditions for {product_category} over {timeframe}:\n",
        "\n",
        "            Provide:\n",
        "            1. Market trends and growth projections\n",
        "            2. Consumer behavior insights\n",
        "            3. Regulatory considerations\n",
        "            4. Technology impacts\n",
        "            5. Strategic recommendations\n",
        "\n",
        "            Focus on actionable insights for retail pricing and inventory.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"market_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"market_analysis\": response.content,\n",
        "                \"timeframe\": timeframe,\n",
        "                \"category\": product_category\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"market_analyzer\"}\n",
        "\n",
        "    def _calculate_ltv(self, purchase_history: List) -> float:\n",
        "        \"\"\"Helper function to calculate customer lifetime value.\"\"\"\n",
        "        if not purchase_history:\n",
        "            return 0.0\n",
        "        total_revenue = sum(p.get('revenue', 0) for p in purchase_history)\n",
        "        purchase_count = len(purchase_history)\n",
        "        avg_order_value = total_revenue / purchase_count if purchase_count > 0 else 0\n",
        "        return min(round(avg_order_value * purchase_count * 0.1, 2), 1000.0)\n",
        "\n",
        "print(\"üõ†Ô∏è Initializing MCP Server...\")\n",
        "mcp_server = RetailMCPServer()\n",
        "\n",
        "\n",
        "# Step 5: A2A (Agent-to-Agent) Communication Layer\n",
        "class A2ACommunicationLayer:\n",
        "    \"\"\"\n",
        "    Manages communication between agents, including direct messages, broadcasts,\n",
        "    and consensus requests, with full tracing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agent_registries = {}\n",
        "        self.conversation_history = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    @traceable\n",
        "    async def register_agent(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        \"\"\"Register an agent in the A2A network.\"\"\"\n",
        "        self.agent_registries[agent_id] = {\n",
        "            \"agent_type\": agent_type,\n",
        "            \"capabilities\": capabilities,\n",
        "            \"status\": \"active\",\n",
        "            \"last_heartbeat\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.performance_metrics[agent_id] = {\n",
        "            \"messages_sent\": 0, \"messages_received\": 0,\n",
        "            \"response_time_avg\": 0.0, \"error_rate\": 0.0\n",
        "        }\n",
        "        print(f\"‚úÖ Agent registered: {agent_id} ({agent_type})\")\n",
        "\n",
        "    @traceable\n",
        "    async def send_message(self, from_agent: str, to_agent: str, message: Dict, message_type: str = \"direct\"):\n",
        "        \"\"\"Send a message between agents with tracing.\"\"\"\n",
        "        try:\n",
        "            message_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.now().isoformat()\n",
        "\n",
        "            message_package = {\n",
        "                \"message_id\": message_id, \"from_agent\": from_agent,\n",
        "                \"to_agent\": to_agent, \"message_type\": message_type,\n",
        "                \"content\": message, \"timestamp\": timestamp\n",
        "            }\n",
        "\n",
        "            if from_agent not in self.conversation_history:\n",
        "                self.conversation_history[from_agent] = []\n",
        "            self.conversation_history[from_agent].append(message_package)\n",
        "\n",
        "            self.performance_metrics[from_agent][\"messages_sent\"] += 1\n",
        "            if to_agent in self.performance_metrics:\n",
        "                self.performance_metrics[to_agent][\"messages_received\"] += 1\n",
        "\n",
        "            print(f\"üì® A2A Message: {from_agent} ‚Üí {to_agent} ({message_type})\")\n",
        "            return message_id\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Message Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @traceable\n",
        "    async def broadcast_message(self, from_agent: str, message: Dict, agent_filter: List[str] = None):\n",
        "        \"\"\"Broadcast a message to multiple agents.\"\"\"\n",
        "        try:\n",
        "            target_agents = agent_filter if agent_filter else list(self.agent_registries.keys())\n",
        "            target_agents = [agent for agent in target_agents if agent != from_agent]\n",
        "\n",
        "            tasks = [self.send_message(from_agent, agent_id, message, \"broadcast\") for agent_id in target_agents]\n",
        "            message_ids = await asyncio.gather(*tasks)\n",
        "            return message_ids\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Broadcast Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_communication_metrics(self) -> Dict:\n",
        "        \"\"\"Get A2A communication performance metrics.\"\"\"\n",
        "        total_messages = sum(m[\"messages_sent\"] for m in self.performance_metrics.values())\n",
        "        active_agents = len([a for a, i in self.agent_registries.items() if i[\"status\"] == \"active\"])\n",
        "        return {\n",
        "            \"total_agents_registered\": len(self.agent_registries),\n",
        "            \"active_agents\": active_agents,\n",
        "            \"total_messages_exchanged\": total_messages,\n",
        "            \"agent_performance\": self.performance_metrics,\n",
        "            \"system_health\": \"optimal\" if active_agents > 0 else \"degraded\"\n",
        "        }\n",
        "\n",
        "print(\"üîÑ Initializing A2A Communication Layer...\")\n",
        "a2a_communication = A2ACommunicationLayer()\n",
        "\n",
        "\n",
        "# Step 6: Enhanced Vector Database with Observability\n",
        "class ObservableVectorDB:\n",
        "    \"\"\"\n",
        "    A wrapper for a Chroma vector database that adds observability and performance tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_store = None\n",
        "        self.query_metrics = {\"total_queries\": 0, \"successful_queries\": 0, \"average_response_time\": 0.0}\n",
        "        self.setup_sample_data()\n",
        "\n",
        "    @traceable\n",
        "    def setup_sample_data(self):\n",
        "        \"\"\"Initialize with comprehensive retail sample data.\"\"\"\n",
        "        sample_docs = [\n",
        "            Document(page_content=\"Premium pricing strategy works best for electronics with 40-60% markup\", metadata={\"category\": \"pricing\", \"product_type\": \"electronics\"}),\n",
        "            Document(page_content=\"Competitive pricing essential for smartphone accessories market\", metadata={\"category\": \"pricing\", \"product_type\": \"accessories\"}),\n",
        "            Document(page_content=\"Safety stock levels should cover 2-week demand for fast-moving goods\", metadata={\"category\": \"inventory\", \"metric\": \"safety_stock\"}),\n",
        "            Document(page_content=\"Holiday season demand increases 60-80% for electronics\", metadata={\"category\": \"trends\", \"season\": \"Q4\"}),\n",
        "            Document(page_content=\"Price sensitivity is highest in the budget segment\", metadata={\"category\": \"customer\", \"insight\": \"pricing\"})\n",
        "        ]\n",
        "        self.vector_store = Chroma.from_documents(documents=sample_docs, embedding=self.embeddings, persist_directory=\"./chroma_retail_db\")\n",
        "        print(\"‚úÖ Observable Vector database initialized\")\n",
        "\n",
        "    @traceable\n",
        "    async def enhanced_search(self, query: str, category: str = None, k: int = 3, agent_id: str = \"unknown\") -> List[Dict]:\n",
        "        \"\"\"Enhanced semantic search with observability.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.query_metrics[\"total_queries\"] += 1\n",
        "        try:\n",
        "            filter_dict = {\"category\": category} if category else {}\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.query_metrics[\"successful_queries\"] += 1\n",
        "            total_queries = self.query_metrics[\"successful_queries\"]\n",
        "            self.query_metrics[\"average_response_time\"] = ((self.query_metrics[\"average_response_time\"] * (total_queries - 1)) + response_time) / total_queries\n",
        "\n",
        "            formatted_results = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"similarity_score\": float(score)} for doc, score in results]\n",
        "            print(f\"üîç VectorDB Search: '{query}' ‚Üí {len(formatted_results)} results\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå VectorDB Search Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Get vector database performance metrics.\"\"\"\n",
        "        success_rate = self.query_metrics[\"successful_queries\"] / max(1, self.query_metrics[\"total_queries\"])\n",
        "        return {**self.query_metrics, \"success_rate\": success_rate, \"health_status\": \"optimal\"}\n",
        "\n",
        "print(\"üõ¢Ô∏è Initializing Observable Vector Database...\")\n",
        "observable_vector_db = ObservableVectorDB()\n",
        "\n",
        "\n",
        "# Step 7: Enhanced Base Agent with MCP and A2A Integration\n",
        "class EnhancedRetailAgent:\n",
        "    \"\"\"\n",
        "    A base class for retail AI agents, integrating MCP tools, A2A communication,\n",
        "    and an observable vector database.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str, agent_type: str, model: str = \"gpt-4\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = ChatOpenAI(model=model, temperature=0.1)\n",
        "        self.mcp_server = mcp_server\n",
        "        self.a2a = a2a_communication\n",
        "        self.vector_db = observable_vector_db\n",
        "        self.history = []\n",
        "        self.performance_metrics = {\n",
        "            \"tasks_completed\": 0, \"mcp_tools_used\": 0,\n",
        "            \"a2a_messages_sent\": 0, \"vector_db_queries\": 0,\n",
        "            \"avg_response_time\": 0.0, \"success_rate\": 1.0\n",
        "        }\n",
        "\n",
        "    async def register(self):\n",
        "        \"\"\"Register agent with the A2A communication layer.\"\"\"\n",
        "        await self.a2a.register_agent(self.agent_id, self.agent_type, self._get_agent_capabilities())\n",
        "\n",
        "    def _get_agent_capabilities(self) -> List[str]:\n",
        "        \"\"\"Define agent capabilities for A2A registration.\"\"\"\n",
        "        caps = [\"analysis\", \"decision_support\", \"collaboration\"]\n",
        "        if \"pricing\" in self.agent_type: caps.extend([\"price_optimization\", \"competitive_analysis\"])\n",
        "        elif \"inventory\" in self.agent_type: caps.extend([\"demand_forecasting\", \"stock_optimization\"])\n",
        "        return caps\n",
        "\n",
        "    @traceable\n",
        "    async def use_mcp_tool(self, tool_name: str, tool_params: Dict) -> Dict:\n",
        "        \"\"\"Use MCP tools with built-in observability and A2A notifications.\"\"\"\n",
        "        if tool_name in self.mcp_server.tools:\n",
        "            self.performance_metrics[\"mcp_tools_used\"] += 1\n",
        "            result = await self.mcp_server.tools[tool_name](**tool_params)\n",
        "\n",
        "            # Notify other agents about tool usage\n",
        "            await self.a2a.broadcast_message(self.agent_id, {\n",
        "                \"type\": \"mcp_tool_used\", \"tool\": tool_name,\n",
        "                \"summary\": str(result)[:100] + \"...\"\n",
        "            })\n",
        "            return result\n",
        "        return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "    def get_enhanced_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive agent status with observability metrics.\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"a2a_registered\": self.agent_id in self.a2a.agent_registries,\n",
        "            \"health_status\": \"optimal\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 8: Enhanced Specialized Agents\n",
        "class EnhancedPricingAgent(EnhancedRetailAgent):\n",
        "    \"\"\"A specialized agent for pricing strategy, using all system components.\"\"\"\n",
        "    def __init__(self, agent_id: str):\n",
        "        super().__init__(agent_id, \"enhanced_pricing_strategist\", \"gpt-4\")\n",
        "\n",
        "    @traceable\n",
        "    async def analyze_pricing(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"Enhanced pricing analysis workflow.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        try:\n",
        "            # 1. Get context from Vector DB\n",
        "            self.performance_metrics[\"vector_db_queries\"] += 1\n",
        "            context = await self.vector_db.enhanced_search(\n",
        "                f\"pricing strategy for {product_data.get('category')}\",\n",
        "                category=\"pricing\", agent_id=self.agent_id\n",
        "            )\n",
        "\n",
        "            # 2. Use MCP tools for advanced analysis\n",
        "            mcp_analysis = await self.use_mcp_tool(\"price_optimizer\", {\n",
        "                \"product_data\": product_data,\n",
        "                \"market_conditions\": {**market_conditions, \"vector_context\": context}\n",
        "            })\n",
        "\n",
        "            # 3. Generate final recommendation\n",
        "            final_recommendation = await self._generate_final_recommendation(\n",
        "                product_data, mcp_analysis, context\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": final_recommendation,\n",
        "                \"mcp_analysis_used\": mcp_analysis,\n",
        "                \"context_used\": context,\n",
        "                \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    async def _generate_final_recommendation(self, product_data: Dict, mcp_analysis: Dict, context: List) -> str:\n",
        "        \"\"\"Synthesize a final recommendation using an LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Synthesize a final pricing recommendation based on the following data:\n",
        "        PRODUCT: {product_data}\n",
        "        MCP ANALYSIS: {mcp_analysis}\n",
        "        VECTOR DB CONTEXT: {context}\n",
        "\n",
        "        Provide a comprehensive pricing strategy for executive decision-making.\n",
        "        \"\"\"\n",
        "        response = await self.llm.ainvoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "\n",
        "# Step 9: Enhanced Orchestrator with Full Observability\n",
        "class ObservableRetailOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the agent army, runs workflows, and provides a centralized\n",
        "    view of system-wide observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.workflow_history = []\n",
        "        self.observability_metrics = {\n",
        "            \"total_workflows\": 0, \"successful_workflows\": 0, \"average_workflow_time\": 0.0\n",
        "        }\n",
        "\n",
        "    @traceable\n",
        "    async def setup_enhanced_agent_army(self):\n",
        "        \"\"\"Initialize the enhanced agent army and register them.\"\"\"\n",
        "        self.agents = {\n",
        "            \"pricing_alpha_enhanced\": EnhancedPricingAgent(\"pricing_alpha_enhanced\"),\n",
        "            \"pricing_beta_enhanced\": EnhancedPricingAgent(\"pricing_beta_enhanced\"),\n",
        "        }\n",
        "        # Register agents explicitly\n",
        "        for agent in self.agents.values():\n",
        "            await agent.register()\n",
        "\n",
        "        print(f\"‚úÖ Enhanced Agent Army Initialized and Registered: {len(self.agents)} observable agents\")\n",
        "\n",
        "    @traceable\n",
        "    async def run_observable_pricing_workflow(self, product_data: Dict) -> Dict:\n",
        "        \"\"\"Run a pricing workflow with full observability.\"\"\"\n",
        "        workflow_start = datetime.now()\n",
        "        self.observability_metrics[\"total_workflows\"] += 1\n",
        "        try:\n",
        "            print(f\"üöÄ Starting OBSERVABLE pricing workflow for {product_data.get('name')}...\")\n",
        "            market_context = await observable_vector_db.enhanced_search(f\"market analysis for {product_data.get('category')}\", k=3)\n",
        "\n",
        "            pricing_agents = [agent for aid, agent in self.agents.items() if \"pricing\" in aid]\n",
        "            tasks = [agent.analyze_pricing(product_data, {\"context\": market_context}) for agent in pricing_agents]\n",
        "            pricing_results = await asyncio.gather(*tasks)\n",
        "\n",
        "            aggregated_recommendation = await self.aggregate_enhanced_recommendations(pricing_results, product_data)\n",
        "\n",
        "            workflow_time = (datetime.now() - workflow_start).total_seconds()\n",
        "            self.observability_metrics[\"successful_workflows\"] += 1\n",
        "            total_workflows = self.observability_metrics[\"successful_workflows\"]\n",
        "            self.observability_metrics[\"average_workflow_time\"] = ((self.observability_metrics[\"average_workflow_time\"] * (total_workflows-1)) + workflow_time) / total_workflows\n",
        "\n",
        "            workflow_result = {\n",
        "                \"workflow_type\": \"observable_pricing\",\n",
        "                \"aggregated_recommendation\": aggregated_recommendation,\n",
        "                \"workflow_duration\": workflow_time\n",
        "            }\n",
        "            self.workflow_history.append(workflow_result)\n",
        "            print(f\"‚úÖ Observable pricing workflow completed in {workflow_time:.2f}s\")\n",
        "            return workflow_result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Observable workflow error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @traceable\n",
        "    async def aggregate_enhanced_recommendations(self, results: List[Dict], context_data: Dict) -> Dict:\n",
        "        \"\"\"Aggregate recommendations from multiple agents using an LLM.\"\"\"\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "        prompt = f\"\"\"\n",
        "        Aggregate these pricing recommendations from multiple AI agents:\n",
        "        CONTEXT: {context_data}\n",
        "        AGENT ANALYSES: {json.dumps(results, indent=2)}\n",
        "        Provide a single, consolidated strategy.\n",
        "        \"\"\"\n",
        "        response = await llm.ainvoke(prompt)\n",
        "        return {\"consolidated_strategy\": response.content}\n",
        "\n",
        "    def get_comprehensive_observability(self) -> Dict:\n",
        "        \"\"\"Get comprehensive observability data from all system components.\"\"\"\n",
        "        agent_statuses = {aid: agent.get_enhanced_status() for aid, agent in self.agents.items()}\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"orchestrator_metrics\": self.observability_metrics,\n",
        "            \"agent_army_status\": {\n",
        "                \"total_agents\": len(self.agents),\n",
        "                \"agent_breakdown\": {\"pricing_agents\": len([a for a in self.agents if \"pricing\" in a])}\n",
        "            },\n",
        "            \"a2a_communication_metrics\": a2a_communication.get_communication_metrics(),\n",
        "            \"vector_db_performance\": observable_vector_db.get_performance_metrics(),\n",
        "            \"workflow_history_summary\": {\n",
        "                \"total_workflows\": len(self.workflow_history),\n",
        "                \"success_rate\": self.observability_metrics[\"successful_workflows\"] / max(1, self.observability_metrics[\"total_workflows\"])\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 10: Generate Sample Data\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data for the demonstration.\"\"\"\n",
        "    return {\n",
        "        \"products\": [\n",
        "            {\"product_id\": \"PROD_001\", \"name\": \"Wireless Bluetooth Headphones\", \"category\": \"electronics\", \"cost_price\": 45.0, \"current_price\": 99.99},\n",
        "            {\"product_id\": \"PROD_002\", \"name\": \"Smartphone Protective Case\", \"category\": \"accessories\", \"cost_price\": 4.50, \"current_price\": 19.99}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"üìä Generating sample data...\")\n",
        "sample_data = generate_sample_data()\n",
        "\n",
        "# Step 11: Main Execution Function\n",
        "async def main():\n",
        "    \"\"\"Initializes and runs the entire demo.\"\"\"\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
        "        print(\"\\n\\n‚ùå Cannot run demo. Please set your OpenAI API key at the top of the script.\")\n",
        "        return\n",
        "\n",
        "    # Initialize Enhanced Orchestrator\n",
        "    print(\"üéØ Initializing Observable Retail Agent Army Orchestrator...\")\n",
        "    observable_orchestrator = ObservableRetailOrchestrator()\n",
        "    await observable_orchestrator.setup_enhanced_agent_army()\n",
        "\n",
        "\n",
        "    # Run Enhanced Demo with Full Observability\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run pricing workflows\n",
        "    for product in sample_data['products']:\n",
        "        await observable_orchestrator.run_observable_pricing_workflow(product)\n",
        "\n",
        "    # Show final dashboard data\n",
        "    final_observability = observable_orchestrator.get_comprehensive_observability()\n",
        "    print(\"\\n3. üìà FINAL OBSERVABILITY DASHBOARD DATA\")\n",
        "    print(json.dumps(final_observability, indent=2, default=str))\n",
        "\n",
        "    # Visualization and final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéä DEMO COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîç OBSERVABILITY FEATURES DEMONSTRATED:\")\n",
        "    print(\"  ‚úÖ MCP (Model Context Protocol) Integration\")\n",
        "    print(\"  ‚úÖ A2A (Agent-to-Agent) Communication Layer\")\n",
        "    print(\"  ‚úÖ LangSmith AI Observability & Tracing\")\n",
        "    print(\"  ‚úÖ Real-time Performance Monitoring\")\n",
        "\n",
        "# Step 12: Run the Demo\n",
        "# In a Jupyter/Colab notebook, this final block should be in its own cell.\n",
        "# The `asyncio.run()` function starts the event loop and runs the `main` coroutine.\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(main())\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the demo run: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkOTrAu0NJTe",
        "outputId": "0881d6c7-a780-44de-db95-42518ecbe826"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Key packages already installed.\n",
            "üìö Libraries imported successfully!\n",
            "üîë Setting up API Keys and Observability...\n",
            "‚úÖ LangSmith client initialized successfully!\n",
            "‚úÖ OpenAI API key verified successfully!\n",
            "üõ†Ô∏è Initializing MCP Server...\n",
            "üîÑ Initializing A2A Communication Layer...\n",
            "üõ¢Ô∏è Initializing Observable Vector Database...\n",
            "‚úÖ Observable Vector database initialized\n",
            "üìä Generating sample data...\n",
            "üéØ Initializing Observable Retail Agent Army Orchestrator...\n",
            "‚úÖ Agent registered: pricing_alpha_enhanced (enhanced_pricing_strategist)\n",
            "‚úÖ Agent registered: pricing_beta_enhanced (enhanced_pricing_strategist)\n",
            "‚úÖ Enhanced Agent Army Initialized and Registered: 2 observable agents\n",
            "\n",
            "======================================================================\n",
            "üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\n",
            "======================================================================\n",
            "üöÄ Starting OBSERVABLE pricing workflow for Wireless Bluetooth Headphones...\n",
            "‚ùå VectorDB Search Error: Expected where to have exactly one operator, got {} in query.\n",
            "üîç VectorDB Search: 'pricing strategy for electronics' ‚Üí 3 results\n",
            "üîç VectorDB Search: 'pricing strategy for electronics' ‚Üí 3 results\n",
            "üì® A2A Message: pricing_beta_enhanced ‚Üí pricing_alpha_enhanced (broadcast)\n",
            "üì® A2A Message: pricing_alpha_enhanced ‚Üí pricing_beta_enhanced (broadcast)\n",
            "‚úÖ Observable pricing workflow completed in 50.54s\n",
            "üöÄ Starting OBSERVABLE pricing workflow for Smartphone Protective Case...\n",
            "‚ùå VectorDB Search Error: Expected where to have exactly one operator, got {} in query.\n",
            "üîç VectorDB Search: 'pricing strategy for accessories' ‚Üí 3 results\n",
            "üîç VectorDB Search: 'pricing strategy for accessories' ‚Üí 3 results\n",
            "üì® A2A Message: pricing_beta_enhanced ‚Üí pricing_alpha_enhanced (broadcast)\n",
            "üì® A2A Message: pricing_alpha_enhanced ‚Üí pricing_beta_enhanced (broadcast)\n",
            "‚úÖ Observable pricing workflow completed in 44.70s\n",
            "\n",
            "3. üìà FINAL OBSERVABILITY DASHBOARD DATA\n",
            "{\n",
            "  \"timestamp\": \"2025-10-10T08:22:34.983992\",\n",
            "  \"orchestrator_metrics\": {\n",
            "    \"total_workflows\": 2,\n",
            "    \"successful_workflows\": 2,\n",
            "    \"average_workflow_time\": 47.6221675\n",
            "  },\n",
            "  \"agent_army_status\": {\n",
            "    \"total_agents\": 2,\n",
            "    \"agent_breakdown\": {\n",
            "      \"pricing_agents\": 2\n",
            "    }\n",
            "  },\n",
            "  \"a2a_communication_metrics\": {\n",
            "    \"total_agents_registered\": 2,\n",
            "    \"active_agents\": 2,\n",
            "    \"total_messages_exchanged\": 4,\n",
            "    \"agent_performance\": {\n",
            "      \"pricing_alpha_enhanced\": {\n",
            "        \"messages_sent\": 2,\n",
            "        \"messages_received\": 2,\n",
            "        \"response_time_avg\": 0.0,\n",
            "        \"error_rate\": 0.0\n",
            "      },\n",
            "      \"pricing_beta_enhanced\": {\n",
            "        \"messages_sent\": 2,\n",
            "        \"messages_received\": 2,\n",
            "        \"response_time_avg\": 0.0,\n",
            "        \"error_rate\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"system_health\": \"optimal\"\n",
            "  },\n",
            "  \"vector_db_performance\": {\n",
            "    \"total_queries\": 6,\n",
            "    \"successful_queries\": 4,\n",
            "    \"average_response_time\": 0.1955585,\n",
            "    \"success_rate\": 0.6666666666666666,\n",
            "    \"health_status\": \"optimal\"\n",
            "  },\n",
            "  \"workflow_history_summary\": {\n",
            "    \"total_workflows\": 2,\n",
            "    \"success_rate\": 1.0\n",
            "  }\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "üéä DEMO COMPLETED!\n",
            "======================================================================\n",
            "üîç OBSERVABILITY FEATURES DEMONSTRATED:\n",
            "  ‚úÖ MCP (Model Context Protocol) Integration\n",
            "  ‚úÖ A2A (Agent-to-Agent) Communication Layer\n",
            "  ‚úÖ LangSmith AI Observability & Tracing\n",
            "  ‚úÖ Real-time Performance Monitoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHF4KsgBOGbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Hajmo0kOGn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "# Note: In a local environment, you would run these commands in your terminal.\n",
        "# In a Jupyter/Colab notebook, the '!' prefix executes them as shell commands.\n",
        "try:\n",
        "    # Check for essential packages\n",
        "    import langchain, chromadb, sentence_transformers, pandas, langsmith, mcp, nest_asyncio\n",
        "    print(\"‚úÖ Key packages already installed.\")\n",
        "except ImportError:\n",
        "    print(\"‚è≥ Installing required packages...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    # Install the full list of packages, including nest_asyncio to handle event loop issues.\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain\", \"langchain-openai\", \"langchain-community\", \"chromadb\", \"sentence-transformers\", \"lightgbm\", \"pandas\", \"numpy\", \"matplotlib\", \"scikit-learn\", \"aiohttp\", \"langsmith\", \"guardrails-ai\", \"nest_asyncio\"])\n",
        "    print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply the patch to allow nested event loops, which is common in notebooks.\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# LangSmith for AI Observability\n",
        "import langsmith\n",
        "from langsmith import Client, traceable\n",
        "\n",
        "# MCP (Model Context Protocol) conceptual import\n",
        "import mcp\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "# Step 3: Configure API Keys and Observability\n",
        "print(\"üîë Setting up API Keys and Observability...\")\n",
        "\n",
        "# It's highly recommended to use environment variables or a secrets management tool.\n",
        "# For this example, we'll use placeholders.\n",
        "# IMPORTANT: Replace these with your actual keys.\n",
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', \"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\")\n",
        "LANGSMITH_API_KEY = os.environ.get('LANGSMITH_API_KEY', \"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\")\n",
        "LANGSMITH_PROJECT = os.environ.get('LANGSMITH_PROJECT', \"retail-agents-MCP\")\n",
        "\n",
        "\n",
        "# Set environment variables for LangChain and LangSmith\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" # Enables LangSmith tracing\n",
        "\n",
        "# Initialize LangSmith client\n",
        "langsmith_client = None\n",
        "try:\n",
        "    if LANGSMITH_API_KEY != \"YOUR_LANGSMITH_API_KEY\":\n",
        "        langsmith_client = Client()\n",
        "        print(\"‚úÖ LangSmith client initialized successfully!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è LangSmith API key not set. Skipping client initialization.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è LangSmith initialization warning: {e}\")\n",
        "\n",
        "# Test the OpenAI API key\n",
        "try:\n",
        "    if OPENAI_API_KEY != \"YOUR_OPENAI_API_KEY\":\n",
        "        test_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        test_response = test_llm.invoke(\"Test\")\n",
        "        print(\"‚úÖ OpenAI API key verified successfully!\")\n",
        "    else:\n",
        "         print(\"‚ùå OpenAI API key is not set. Please replace 'YOUR_OPENAI_API_KEY'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI API key error: {e}\")\n",
        "\n",
        "\n",
        "# Step 4: MCP Server Implementation for Retail\n",
        "class RetailMCPServer:\n",
        "    \"\"\"\n",
        "    A simulated Model Context Protocol (MCP) server that provides specialized tools\n",
        "    for retail analysis. Each tool is traceable for observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"price_optimizer\": self.price_optimization_tool,\n",
        "            \"demand_forecaster\": self.demand_forecasting_tool,\n",
        "            \"inventory_optimizer\": self.inventory_optimization_tool,\n",
        "            \"customer_analyzer\": self.customer_analytics_tool,\n",
        "            \"competitor_monitor\": self.competitive_intelligence_tool,\n",
        "            \"market_analyzer\": self.market_analysis_tool\n",
        "        }\n",
        "        self.session_id = str(uuid.uuid4())\n",
        "\n",
        "    @traceable\n",
        "    async def price_optimization_tool(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced price optimization using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Perform advanced price optimization analysis:\n",
        "\n",
        "            PRODUCT: {product_data}\n",
        "            MARKET: {market_conditions}\n",
        "\n",
        "            Calculate:\n",
        "            1. Price elasticity estimation\n",
        "            2. Optimal price point with confidence intervals\n",
        "            3. Competitive positioning strategy\n",
        "            4. Revenue maximization approach\n",
        "            5. Risk assessment\n",
        "\n",
        "            Return structured JSON analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"price_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": response.content,\n",
        "                \"recommendations\": {\n",
        "                    \"optimal_price_range\": {\"min\": 45.0, \"max\": 125.0, \"recommended\": 89.99},\n",
        "                    \"confidence_score\": 0.87,\n",
        "                    \"expected_revenue_impact\": \"+15-25%\",\n",
        "                    \"risk_level\": \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"price_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def demand_forecasting_tool(self, historical_data: List, market_signals: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Demand forecasting with market signals.\"\"\"\n",
        "        try:\n",
        "            forecast_data = {\n",
        "                \"next_30_days\": np.random.normal(100, 20, 30).tolist(),\n",
        "                \"confidence_interval\": {\"lower\": 80, \"upper\": 120},\n",
        "                \"seasonality_factor\": 1.15,\n",
        "                \"trend_direction\": \"upward\"\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"demand_forecaster\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"forecast\": forecast_data,\n",
        "                \"market_signals_used\": list(market_signals.keys()),\n",
        "                \"data_points_analyzed\": len(historical_data)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"demand_forecaster\"}\n",
        "\n",
        "    @traceable\n",
        "    async def inventory_optimization_tool(self, current_stock: Dict, demand_forecast: Dict) -> Dict:\n",
        "        \"\"\"MCP Tool: Inventory optimization based on demand forecasting.\"\"\"\n",
        "        try:\n",
        "            optimization_recommendations = {\n",
        "                \"safety_stock_levels\": {\n",
        "                    \"high_demand\": current_stock.get('current_stock', 0) * 0.3,\n",
        "                    \"medium_demand\": current_stock.get('current_stock', 0) * 0.2,\n",
        "                    \"low_demand\": current_stock.get('current_stock', 0) * 0.1\n",
        "                },\n",
        "                \"reorder_points\": {\n",
        "                    \"urgent\": current_stock.get('reorder_point', 0) * 0.8,\n",
        "                    \"standard\": current_stock.get('reorder_point', 0),\n",
        "                    \"relaxed\": current_stock.get('reorder_point', 0) * 1.2\n",
        "                },\n",
        "                \"cost_optimization\": {\n",
        "                    \"expected_savings\": \"15-20%\",\n",
        "                    \"stockout_risk_reduction\": \"40-60%\",\n",
        "                    \"carrying_cost_optimization\": \"10-15%\"\n",
        "                }\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"inventory_optimizer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"optimization\": optimization_recommendations,\n",
        "                \"demand_forecast_incorporated\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"inventory_optimizer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def customer_analytics_tool(self, customer_data: Dict, purchase_history: List) -> Dict:\n",
        "        \"\"\"MCP Tool: Advanced customer analytics using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze customer behavior and generate insights:\n",
        "\n",
        "            CUSTOMER: {customer_data}\n",
        "            PURCHASE HISTORY: {purchase_history[:5]}\n",
        "\n",
        "            Provide:\n",
        "            1. Customer lifetime value prediction\n",
        "            2. Churn risk assessment\n",
        "            3. Personalization opportunities\n",
        "            4. Retention strategy\n",
        "            5. Cross-selling recommendations\n",
        "\n",
        "            Return structured analysis.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"customer_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"customer_insights\": response.content,\n",
        "                \"metrics\": {\n",
        "                    \"lifetime_value_score\": self._calculate_ltv(purchase_history),\n",
        "                    \"churn_risk\": \"low\" if len(purchase_history) > 3 else \"medium\",\n",
        "                    \"engagement_level\": \"high\" if len(purchase_history) > 5 else \"medium\"\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"customer_analyzer\"}\n",
        "\n",
        "    @traceable\n",
        "    async def competitive_intelligence_tool(self, product_data: Dict, market_scope: str = \"local\") -> Dict:\n",
        "        \"\"\"MCP Tool: Competitive market intelligence.\"\"\"\n",
        "        try:\n",
        "            competitor_analysis = {\n",
        "                \"main_competitors\": [\"Competitor A\", \"Competitor B\", \"Competitor C\"],\n",
        "                \"price_comparison\": {\n",
        "                    \"our_price\": product_data.get('current_price', 0),\n",
        "                    \"competitor_avg\": product_data.get('current_price', 0) * 0.9,\n",
        "                    \"market_range\": {\n",
        "                        \"min\": product_data.get('current_price', 0) * 0.7,\n",
        "                        \"max\": product_data.get('current_price', 0) * 1.3\n",
        "                    }\n",
        "                },\n",
        "                \"competitive_positioning\": \"market_leader\" if product_data.get('current_price', 0) > 100 else \"value_player\",\n",
        "                \"recommendations\": [\n",
        "                    \"Monitor competitor pricing weekly\",\n",
        "                    \"Differentiate through value-added services\",\n",
        "                    \"Consider bundle pricing strategies\"\n",
        "                ]\n",
        "            }\n",
        "            return {\n",
        "                \"tool\": \"competitor_monitor\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"analysis\": competitor_analysis,\n",
        "                \"market_scope\": market_scope\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"competitor_monitor\"}\n",
        "\n",
        "    @traceable\n",
        "    async def market_analysis_tool(self, product_category: str, timeframe: str = \"30d\") -> Dict:\n",
        "        \"\"\"MCP Tool: Comprehensive market analysis using an LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
        "            prompt = f\"\"\"\n",
        "            Analyze market conditions for {product_category} over {timeframe}:\n",
        "\n",
        "            Provide:\n",
        "            1. Market trends and growth projections\n",
        "            2. Consumer behavior insights\n",
        "            3. Regulatory considerations\n",
        "            4. Technology impacts\n",
        "            5. Strategic recommendations\n",
        "\n",
        "            Focus on actionable insights for retail pricing and inventory.\n",
        "            \"\"\"\n",
        "            response = await llm.ainvoke(prompt)\n",
        "            return {\n",
        "                \"tool\": \"market_analyzer\",\n",
        "                \"session_id\": self.session_id,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"market_analysis\": response.content,\n",
        "                \"timeframe\": timeframe,\n",
        "                \"category\": product_category\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e), \"tool\": \"market_analyzer\"}\n",
        "\n",
        "    def _calculate_ltv(self, purchase_history: List) -> float:\n",
        "        \"\"\"Helper function to calculate customer lifetime value.\"\"\"\n",
        "        if not purchase_history:\n",
        "            return 0.0\n",
        "        total_revenue = sum(p.get('revenue', 0) for p in purchase_history)\n",
        "        purchase_count = len(purchase_history)\n",
        "        avg_order_value = total_revenue / purchase_count if purchase_count > 0 else 0\n",
        "        return min(round(avg_order_value * purchase_count * 0.1, 2), 1000.0)\n",
        "\n",
        "print(\"üõ†Ô∏è Initializing MCP Server...\")\n",
        "mcp_server = RetailMCPServer()\n",
        "\n",
        "\n",
        "# Step 5: A2A (Agent-to-Agent) Communication Layer\n",
        "class A2ACommunicationLayer:\n",
        "    \"\"\"\n",
        "    Manages communication between agents, including direct messages, broadcasts,\n",
        "    and consensus requests, with full tracing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agent_registries = {}\n",
        "        self.conversation_history = {}\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "    @traceable\n",
        "    async def register_agent(self, agent_id: str, agent_type: str, capabilities: List[str]):\n",
        "        \"\"\"Register an agent in the A2A network.\"\"\"\n",
        "        self.agent_registries[agent_id] = {\n",
        "            \"agent_type\": agent_type,\n",
        "            \"capabilities\": capabilities,\n",
        "            \"status\": \"active\",\n",
        "            \"last_heartbeat\": datetime.now().isoformat()\n",
        "        }\n",
        "        self.performance_metrics[agent_id] = {\n",
        "            \"messages_sent\": 0, \"messages_received\": 0,\n",
        "            \"response_time_avg\": 0.0, \"error_rate\": 0.0\n",
        "        }\n",
        "        print(f\"‚úÖ Agent registered: {agent_id} ({agent_type})\")\n",
        "\n",
        "    @traceable\n",
        "    async def send_message(self, from_agent: str, to_agent: str, message: Dict, message_type: str = \"direct\"):\n",
        "        \"\"\"Send a message between agents with tracing.\"\"\"\n",
        "        try:\n",
        "            message_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.now().isoformat()\n",
        "\n",
        "            message_package = {\n",
        "                \"message_id\": message_id, \"from_agent\": from_agent,\n",
        "                \"to_agent\": to_agent, \"message_type\": message_type,\n",
        "                \"content\": message, \"timestamp\": timestamp\n",
        "            }\n",
        "\n",
        "            if from_agent not in self.conversation_history:\n",
        "                self.conversation_history[from_agent] = []\n",
        "            self.conversation_history[from_agent].append(message_package)\n",
        "\n",
        "            self.performance_metrics[from_agent][\"messages_sent\"] += 1\n",
        "            if to_agent in self.performance_metrics:\n",
        "                self.performance_metrics[to_agent][\"messages_received\"] += 1\n",
        "\n",
        "            print(f\"üì® A2A Message: {from_agent} ‚Üí {to_agent} ({message_type})\")\n",
        "            return message_id\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Message Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    @traceable\n",
        "    async def broadcast_message(self, from_agent: str, message: Dict, agent_filter: List[str] = None):\n",
        "        \"\"\"Broadcast a message to multiple agents.\"\"\"\n",
        "        try:\n",
        "            target_agents = agent_filter if agent_filter else list(self.agent_registries.keys())\n",
        "            target_agents = [agent for agent in target_agents if agent != from_agent]\n",
        "\n",
        "            tasks = [self.send_message(from_agent, agent_id, message, \"broadcast\") for agent_id in target_agents]\n",
        "            message_ids = await asyncio.gather(*tasks)\n",
        "            return message_ids\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå A2A Broadcast Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_communication_metrics(self) -> Dict:\n",
        "        \"\"\"Get A2A communication performance metrics.\"\"\"\n",
        "        total_messages = sum(m[\"messages_sent\"] for m in self.performance_metrics.values())\n",
        "        active_agents = len([a for a, i in self.agent_registries.items() if i[\"status\"] == \"active\"])\n",
        "        return {\n",
        "            \"total_agents_registered\": len(self.agent_registries),\n",
        "            \"active_agents\": active_agents,\n",
        "            \"total_messages_exchanged\": total_messages,\n",
        "            \"agent_performance\": self.performance_metrics,\n",
        "            \"system_health\": \"optimal\" if active_agents > 0 else \"degraded\"\n",
        "        }\n",
        "\n",
        "print(\"üîÑ Initializing A2A Communication Layer...\")\n",
        "a2a_communication = A2ACommunicationLayer()\n",
        "\n",
        "\n",
        "# Step 6: Enhanced Vector Database with Observability\n",
        "class ObservableVectorDB:\n",
        "    \"\"\"\n",
        "    A wrapper for a Chroma vector database that adds observability and performance tracking.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "        self.vector_store = None\n",
        "        self.query_metrics = {\"total_queries\": 0, \"successful_queries\": 0, \"average_response_time\": 0.0}\n",
        "        self.setup_sample_data()\n",
        "\n",
        "    @traceable\n",
        "    def setup_sample_data(self):\n",
        "        \"\"\"Initialize with comprehensive retail sample data.\"\"\"\n",
        "        sample_docs = [\n",
        "            Document(page_content=\"Premium pricing strategy works best for electronics with 40-60% markup\", metadata={\"category\": \"pricing\", \"product_type\": \"electronics\"}),\n",
        "            Document(page_content=\"Competitive pricing essential for smartphone accessories market\", metadata={\"category\": \"pricing\", \"product_type\": \"accessories\"}),\n",
        "            Document(page_content=\"Safety stock levels should cover 2-week demand for fast-moving goods\", metadata={\"category\": \"inventory\", \"metric\": \"safety_stock\"}),\n",
        "            Document(page_content=\"Holiday season demand increases 60-80% for electronics\", metadata={\"category\": \"trends\", \"season\": \"Q4\"}),\n",
        "            Document(page_content=\"Price sensitivity is highest in the budget segment\", metadata={\"category\": \"customer\", \"insight\": \"pricing\"})\n",
        "        ]\n",
        "        self.vector_store = Chroma.from_documents(documents=sample_docs, embedding=self.embeddings, persist_directory=\"./chroma_retail_db\")\n",
        "        print(\"‚úÖ Observable Vector database initialized\")\n",
        "\n",
        "    @traceable\n",
        "    async def enhanced_search(self, query: str, category: str = None, k: int = 3, agent_id: str = \"unknown\") -> List[Dict]:\n",
        "        \"\"\"Enhanced semantic search with observability.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.query_metrics[\"total_queries\"] += 1\n",
        "        try:\n",
        "            filter_dict = {\"category\": category} if category else {}\n",
        "            results = self.vector_store.similarity_search_with_score(query, k=k, filter=filter_dict)\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.query_metrics[\"successful_queries\"] += 1\n",
        "            total_queries = self.query_metrics[\"successful_queries\"]\n",
        "            self.query_metrics[\"average_response_time\"] = ((self.query_metrics[\"average_response_time\"] * (total_queries - 1)) + response_time) / total_queries\n",
        "\n",
        "            formatted_results = [{\"content\": doc.page_content, \"metadata\": doc.metadata, \"similarity_score\": float(score)} for doc, score in results]\n",
        "            print(f\"üîç VectorDB Search: '{query}' ‚Üí {len(formatted_results)} results\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå VectorDB Search Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Get vector database performance metrics.\"\"\"\n",
        "        success_rate = self.query_metrics[\"successful_queries\"] / max(1, self.query_metrics[\"total_queries\"])\n",
        "        return {**self.query_metrics, \"success_rate\": success_rate, \"health_status\": \"optimal\"}\n",
        "\n",
        "print(\"üõ¢Ô∏è Initializing Observable Vector Database...\")\n",
        "observable_vector_db = ObservableVectorDB()\n",
        "\n",
        "\n",
        "# Step 7: Enhanced Base Agent with MCP and A2A Integration\n",
        "class EnhancedRetailAgent:\n",
        "    \"\"\"\n",
        "    A base class for retail AI agents, integrating MCP tools, A2A communication,\n",
        "    and an observable vector database.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent_id: str, agent_type: str, model: str = \"gpt-4\"):\n",
        "        self.agent_id = agent_id\n",
        "        self.agent_type = agent_type\n",
        "        self.llm = ChatOpenAI(model=model, temperature=0.1)\n",
        "        self.mcp_server = mcp_server\n",
        "        self.a2a = a2a_communication\n",
        "        self.vector_db = observable_vector_db\n",
        "        self.history = []\n",
        "        self.performance_metrics = {\n",
        "            \"tasks_completed\": 0, \"mcp_tools_used\": 0,\n",
        "            \"a2a_messages_sent\": 0, \"vector_db_queries\": 0,\n",
        "            \"avg_response_time\": 0.0, \"success_rate\": 1.0\n",
        "        }\n",
        "\n",
        "    async def register(self):\n",
        "        \"\"\"Register agent with the A2A communication layer.\"\"\"\n",
        "        await self.a2a.register_agent(self.agent_id, self.agent_type, self._get_agent_capabilities())\n",
        "\n",
        "    def _get_agent_capabilities(self) -> List[str]:\n",
        "        \"\"\"Define agent capabilities for A2A registration.\"\"\"\n",
        "        caps = [\"analysis\", \"decision_support\", \"collaboration\"]\n",
        "        if \"pricing\" in self.agent_type: caps.extend([\"price_optimization\", \"competitive_analysis\"])\n",
        "        elif \"inventory\" in self.agent_type: caps.extend([\"demand_forecasting\", \"stock_optimization\"])\n",
        "        return caps\n",
        "\n",
        "    @traceable\n",
        "    async def use_mcp_tool(self, tool_name: str, tool_params: Dict) -> Dict:\n",
        "        \"\"\"Use MCP tools with built-in observability and A2A notifications.\"\"\"\n",
        "        if tool_name in self.mcp_server.tools:\n",
        "            self.performance_metrics[\"mcp_tools_used\"] += 1\n",
        "            result = await self.mcp_server.tools[tool_name](**tool_params)\n",
        "\n",
        "            # Notify other agents about tool usage\n",
        "            await self.a2a.broadcast_message(self.agent_id, {\n",
        "                \"type\": \"mcp_tool_used\", \"tool\": tool_name,\n",
        "                \"summary\": str(result)[:100] + \"...\"\n",
        "            })\n",
        "            return result\n",
        "        return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "    def get_enhanced_status(self) -> Dict:\n",
        "        \"\"\"Get comprehensive agent status with observability metrics.\"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"agent_type\": self.agent_type,\n",
        "            \"performance_metrics\": self.performance_metrics,\n",
        "            \"a2a_registered\": self.agent_id in self.a2a.agent_registries,\n",
        "            \"health_status\": \"optimal\"\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 8: Enhanced Specialized Agents\n",
        "class EnhancedPricingAgent(EnhancedRetailAgent):\n",
        "    \"\"\"A specialized agent for pricing strategy, using all system components.\"\"\"\n",
        "    def __init__(self, agent_id: str):\n",
        "        super().__init__(agent_id, \"enhanced_pricing_strategist\", \"gpt-4\")\n",
        "\n",
        "    @traceable\n",
        "    async def analyze_pricing(self, product_data: Dict, market_conditions: Dict) -> Dict:\n",
        "        \"\"\"Enhanced pricing analysis workflow.\"\"\"\n",
        "        start_time = datetime.now()\n",
        "        try:\n",
        "            # 1. Get context from Vector DB\n",
        "            self.performance_metrics[\"vector_db_queries\"] += 1\n",
        "            context = await self.vector_db.enhanced_search(\n",
        "                f\"pricing strategy for {product_data.get('category')}\",\n",
        "                category=\"pricing\", agent_id=self.agent_id\n",
        "            )\n",
        "\n",
        "            # 2. Use MCP tools for advanced analysis\n",
        "            mcp_analysis = await self.use_mcp_tool(\"price_optimizer\", {\n",
        "                \"product_data\": product_data,\n",
        "                \"market_conditions\": {**market_conditions, \"vector_context\": context}\n",
        "            })\n",
        "\n",
        "            # 3. Generate final recommendation\n",
        "            final_recommendation = await self._generate_final_recommendation(\n",
        "                product_data, mcp_analysis, context\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"recommendation\": final_recommendation,\n",
        "                \"mcp_analysis_used\": mcp_analysis,\n",
        "                \"context_used\": context,\n",
        "                \"processing_time\": (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    async def _generate_final_recommendation(self, product_data: Dict, mcp_analysis: Dict, context: List) -> str:\n",
        "        \"\"\"Synthesize a final recommendation using an LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Synthesize a final pricing recommendation based on the following data:\n",
        "        PRODUCT: {product_data}\n",
        "        MCP ANALYSIS: {mcp_analysis}\n",
        "        VECTOR DB CONTEXT: {context}\n",
        "\n",
        "        Provide a comprehensive pricing strategy for executive decision-making.\n",
        "        \"\"\"\n",
        "        response = await self.llm.ainvoke(prompt)\n",
        "        return response.content\n",
        "\n",
        "\n",
        "# Step 9: Enhanced Orchestrator with Full Observability\n",
        "class ObservableRetailOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the agent army, runs workflows, and provides a centralized\n",
        "    view of system-wide observability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.agents = {}\n",
        "        self.workflow_history = []\n",
        "        self.observability_metrics = {\n",
        "            \"total_workflows\": 0, \"successful_workflows\": 0, \"average_workflow_time\": 0.0\n",
        "        }\n",
        "\n",
        "    @traceable\n",
        "    async def setup_enhanced_agent_army(self):\n",
        "        \"\"\"Initialize the enhanced agent army and register them.\"\"\"\n",
        "        self.agents = {\n",
        "            \"pricing_alpha_enhanced\": EnhancedPricingAgent(\"pricing_alpha_enhanced\"),\n",
        "            \"pricing_beta_enhanced\": EnhancedPricingAgent(\"pricing_beta_enhanced\"),\n",
        "        }\n",
        "        # Register agents explicitly\n",
        "        for agent in self.agents.values():\n",
        "            await agent.register()\n",
        "\n",
        "        print(f\"‚úÖ Enhanced Agent Army Initialized and Registered: {len(self.agents)} observable agents\")\n",
        "\n",
        "    @traceable\n",
        "    async def run_observable_pricing_workflow(self, product_data: Dict) -> Dict:\n",
        "        \"\"\"Run a pricing workflow with full observability.\"\"\"\n",
        "        workflow_start = datetime.now()\n",
        "        self.observability_metrics[\"total_workflows\"] += 1\n",
        "        try:\n",
        "            print(f\"üöÄ Starting OBSERVABLE pricing workflow for {product_data.get('name')}...\")\n",
        "            market_context = await observable_vector_db.enhanced_search(\n",
        "                f\"market analysis for {product_data.get('category')}\",\n",
        "                category=\"trends\",\n",
        "                k=3\n",
        "            )\n",
        "\n",
        "            pricing_agents = [agent for aid, agent in self.agents.items() if \"pricing\" in aid]\n",
        "            tasks = [agent.analyze_pricing(product_data, {\"context\": market_context}) for agent in pricing_agents]\n",
        "            pricing_results = await asyncio.gather(*tasks)\n",
        "\n",
        "            aggregated_recommendation = await self.aggregate_enhanced_recommendations(pricing_results, product_data)\n",
        "\n",
        "            workflow_time = (datetime.now() - workflow_start).total_seconds()\n",
        "            self.observability_metrics[\"successful_workflows\"] += 1\n",
        "            total_workflows = self.observability_metrics[\"successful_workflows\"]\n",
        "            self.observability_metrics[\"average_workflow_time\"] = ((self.observability_metrics[\"average_workflow_time\"] * (total_workflows-1)) + workflow_time) / total_workflows\n",
        "\n",
        "            workflow_result = {\n",
        "                \"workflow_type\": \"observable_pricing\",\n",
        "                \"aggregated_recommendation\": aggregated_recommendation,\n",
        "                \"workflow_duration\": workflow_time\n",
        "            }\n",
        "            self.workflow_history.append(workflow_result)\n",
        "            print(f\"‚úÖ Observable pricing workflow completed in {workflow_time:.2f}s\")\n",
        "            return workflow_result\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Observable workflow error: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    @traceable\n",
        "    async def aggregate_enhanced_recommendations(self, results: List[Dict], context_data: Dict) -> Dict:\n",
        "        \"\"\"Aggregate recommendations from multiple agents using an LLM.\"\"\"\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "        prompt = f\"\"\"\n",
        "        Aggregate these pricing recommendations from multiple AI agents:\n",
        "        CONTEXT: {context_data}\n",
        "        AGENT ANALYSES: {json.dumps(results, indent=2)}\n",
        "        Provide a single, consolidated strategy.\n",
        "        \"\"\"\n",
        "        response = await llm.ainvoke(prompt)\n",
        "        return {\"consolidated_strategy\": response.content}\n",
        "\n",
        "    def get_comprehensive_observability(self) -> Dict:\n",
        "        \"\"\"Get comprehensive observability data from all system components.\"\"\"\n",
        "        agent_statuses = {aid: agent.get_enhanced_status() for aid, agent in self.agents.items()}\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"orchestrator_metrics\": self.observability_metrics,\n",
        "            \"agent_army_status\": {\n",
        "                \"total_agents\": len(self.agents),\n",
        "                \"agent_breakdown\": {\"pricing_agents\": len([a for a in self.agents if \"pricing\" in a])}\n",
        "            },\n",
        "            \"a2a_communication_metrics\": a2a_communication.get_communication_metrics(),\n",
        "            \"vector_db_performance\": observable_vector_db.get_performance_metrics(),\n",
        "            \"workflow_history_summary\": {\n",
        "                \"total_workflows\": len(self.workflow_history),\n",
        "                \"success_rate\": self.observability_metrics[\"successful_workflows\"] / max(1, self.observability_metrics[\"total_workflows\"])\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "# Step 10: Generate Sample Data\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data for the demonstration.\"\"\"\n",
        "    return {\n",
        "        \"products\": [\n",
        "            {\"product_id\": \"PROD_001\", \"name\": \"Wireless Bluetooth Headphones\", \"category\": \"electronics\", \"cost_price\": 45.0, \"current_price\": 99.99},\n",
        "            {\"product_id\": \"PROD_002\", \"name\": \"Smartphone Protective Case\", \"category\": \"accessories\", \"cost_price\": 4.50, \"current_price\": 19.99}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"üìä Generating sample data...\")\n",
        "sample_data = generate_sample_data()\n",
        "\n",
        "# Step 11: Main Execution Function\n",
        "async def main():\n",
        "    \"\"\"Initializes and runs the entire demo.\"\"\"\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
        "        print(\"\\n\\n‚ùå Cannot run demo. Please set your OpenAI API key at the top of the script.\")\n",
        "        return\n",
        "\n",
        "    # Initialize Enhanced Orchestrator\n",
        "    print(\"üéØ Initializing Observable Retail Agent Army Orchestrator...\")\n",
        "    observable_orchestrator = ObservableRetailOrchestrator()\n",
        "    await observable_orchestrator.setup_enhanced_agent_army()\n",
        "\n",
        "\n",
        "    # Run Enhanced Demo with Full Observability\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run pricing workflows\n",
        "    for product in sample_data['products']:\n",
        "        await observable_orchestrator.run_observable_pricing_workflow(product)\n",
        "\n",
        "    # Show final dashboard data\n",
        "    final_observability = observable_orchestrator.get_comprehensive_observability()\n",
        "    print(\"\\n3. üìà FINAL OBSERVABILITY DASHBOARD DATA\")\n",
        "    print(json.dumps(final_observability, indent=2, default=str))\n",
        "\n",
        "    # Visualization and final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéä DEMO COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üîç OBSERVABILITY FEATURES DEMONSTRATED:\")\n",
        "    print(\"  ‚úÖ MCP (Model Context Protocol) Integration\")\n",
        "    print(\"  ‚úÖ A2A (Agent-to-Agent) Communication Layer\")\n",
        "    print(\"  ‚úÖ LangSmith AI Observability & Tracing\")\n",
        "    print(\"  ‚úÖ Real-time Performance Monitoring\")\n",
        "\n",
        "# Step 12: Run the Demo\n",
        "# This is the standard way to run an async function from a script.\n",
        "# The `nest_asyncio.apply()` at the top handles notebook environments.\n",
        "if __name__ == \"__main__\":\n",
        "    # A simple check to avoid running the demo if keys are not set.\n",
        "    if OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY\" or LANGSMITH_API_KEY == \"YOUR_LANGSMITH_API_KEY\":\n",
        "        print(\"\\nüõë DEMO HALTED: Please provide your API keys in Step 3.\")\n",
        "    else:\n",
        "        asyncio.run(main())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYF00MfCOGrH",
        "outputId": "e1fbca27-f21c-4ca1-f14e-b2c94afeffbe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Key packages already installed.\n",
            "üìö Libraries imported successfully!\n",
            "üîë Setting up API Keys and Observability...\n",
            "‚úÖ LangSmith client initialized successfully!\n",
            "‚úÖ OpenAI API key verified successfully!\n",
            "üõ†Ô∏è Initializing MCP Server...\n",
            "üîÑ Initializing A2A Communication Layer...\n",
            "üõ¢Ô∏è Initializing Observable Vector Database...\n",
            "‚úÖ Observable Vector database initialized\n",
            "üìä Generating sample data...\n",
            "üéØ Initializing Observable Retail Agent Army Orchestrator...\n",
            "‚úÖ Agent registered: pricing_alpha_enhanced (enhanced_pricing_strategist)\n",
            "‚úÖ Agent registered: pricing_beta_enhanced (enhanced_pricing_strategist)\n",
            "‚úÖ Enhanced Agent Army Initialized and Registered: 2 observable agents\n",
            "\n",
            "======================================================================\n",
            "üîç OBSERVABLE RETAIL AI AGENT ARMY - ENHANCED DEMO\n",
            "======================================================================\n",
            "üöÄ Starting OBSERVABLE pricing workflow for Wireless Bluetooth Headphones...\n",
            "üîç VectorDB Search: 'market analysis for electronics' ‚Üí 3 results\n",
            "üîç VectorDB Search: 'pricing strategy for electronics' ‚Üí 3 results\n",
            "üîç VectorDB Search: 'pricing strategy for electronics' ‚Üí 3 results\n",
            "üì® A2A Message: pricing_beta_enhanced ‚Üí pricing_alpha_enhanced (broadcast)\n",
            "üì® A2A Message: pricing_alpha_enhanced ‚Üí pricing_beta_enhanced (broadcast)\n",
            "‚úÖ Observable pricing workflow completed in 47.45s\n",
            "üöÄ Starting OBSERVABLE pricing workflow for Smartphone Protective Case...\n",
            "üîç VectorDB Search: 'market analysis for accessories' ‚Üí 3 results\n",
            "üîç VectorDB Search: 'pricing strategy for accessories' ‚Üí 3 results\n",
            "üîç VectorDB Search: 'pricing strategy for accessories' ‚Üí 3 results\n",
            "üì® A2A Message: pricing_alpha_enhanced ‚Üí pricing_beta_enhanced (broadcast)\n",
            "üì® A2A Message: pricing_beta_enhanced ‚Üí pricing_alpha_enhanced (broadcast)\n",
            "‚úÖ Observable pricing workflow completed in 51.53s\n",
            "\n",
            "3. üìà FINAL OBSERVABILITY DASHBOARD DATA\n",
            "{\n",
            "  \"timestamp\": \"2025-10-10T08:31:43.067296\",\n",
            "  \"orchestrator_metrics\": {\n",
            "    \"total_workflows\": 2,\n",
            "    \"successful_workflows\": 2,\n",
            "    \"average_workflow_time\": 49.4906615\n",
            "  },\n",
            "  \"agent_army_status\": {\n",
            "    \"total_agents\": 2,\n",
            "    \"agent_breakdown\": {\n",
            "      \"pricing_agents\": 2\n",
            "    }\n",
            "  },\n",
            "  \"a2a_communication_metrics\": {\n",
            "    \"total_agents_registered\": 2,\n",
            "    \"active_agents\": 2,\n",
            "    \"total_messages_exchanged\": 4,\n",
            "    \"agent_performance\": {\n",
            "      \"pricing_alpha_enhanced\": {\n",
            "        \"messages_sent\": 2,\n",
            "        \"messages_received\": 2,\n",
            "        \"response_time_avg\": 0.0,\n",
            "        \"error_rate\": 0.0\n",
            "      },\n",
            "      \"pricing_beta_enhanced\": {\n",
            "        \"messages_sent\": 2,\n",
            "        \"messages_received\": 2,\n",
            "        \"response_time_avg\": 0.0,\n",
            "        \"error_rate\": 0.0\n",
            "      }\n",
            "    },\n",
            "    \"system_health\": \"optimal\"\n",
            "  },\n",
            "  \"vector_db_performance\": {\n",
            "    \"total_queries\": 6,\n",
            "    \"successful_queries\": 6,\n",
            "    \"average_response_time\": 0.27841166666666667,\n",
            "    \"success_rate\": 1.0,\n",
            "    \"health_status\": \"optimal\"\n",
            "  },\n",
            "  \"workflow_history_summary\": {\n",
            "    \"total_workflows\": 2,\n",
            "    \"success_rate\": 1.0\n",
            "  }\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "üéä DEMO COMPLETED!\n",
            "======================================================================\n",
            "üîç OBSERVABILITY FEATURES DEMONSTRATED:\n",
            "  ‚úÖ MCP (Model Context Protocol) Integration\n",
            "  ‚úÖ A2A (Agent-to-Agent) Communication Layer\n",
            "  ‚úÖ LangSmith AI Observability & Tracing\n",
            "  ‚úÖ Real-time Performance Monitoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAHA_z1zOGtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K7__alAyOGvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70UFZWzWOGyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}